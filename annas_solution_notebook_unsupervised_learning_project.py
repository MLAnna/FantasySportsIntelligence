# -*- coding: utf-8 -*-
"""Annas_Solution_Notebook_Unsupervised_Learning_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ekq-gwBLpRgMHmmkHLgtr7igs6Io3qCM

# **ANNA LAPUSHNER**


---


**Unsupervised Learning Practice Project Solution:  Fantasy Sports Clustering Analysis**

### Fantasy sports are online gaming platforms where participants draft and manage virtual teams of real professional sports players. Based on the performance of the players in the real world, players are allotted points in the fantasy sports platform every match. The objective is to create the best possible team with a fixed budget to score maximum fantasy points, and users compete against each other over an entire sports league or season. Some of these fantasy sports require actual financial investments for participation, with chances of winning monetary rewards as well as free matchday tickets on a periodic basis.

**According to Mordor Intelligence, the fantasy sports market is expected to be valued at \$32.75 billion in 2024; and may reach $92.52 billlion in 2032 with a CAGR of 14.1% (Straits Research)**.

The fantasy sports market has seen tremendous growth over the past few years, from a valuation of $18.6 billion in 2019, and some factors that may be driving its growth include:
*   The increasing popularity of sports leagues and tournaments around the world
*   The proliferation of internet-enabled devices, especially smartphones, which has expanded the user base for fantasy sports platforms
*   Easy access to fantasy sports apps and websites, which drives user engagement and participation

American Football (specifically the NFL) has outpaced soccer as the segment with the greatest marketshare. Currently, 23% of Americans aged 18+ participate in sports betting. And 19% of Americans aged 18+ participate in fantasy sports.

North America was the largest region in the fantasy sports market in 2023. Asia-Pacific is expected to be the fastest region in the forecast period.

**This market is expected to continue to witness a global surge and reach a \$48.6 billion valuation by 2031.**

----------------------------
## **Objective**
-----------------------------

OnSports is a fantasy sports platform that has fantasy leagues for many different sports and has witnessed an increasing number of participants globally over the past 5 years. For each player, a price is set at the start, and the price keeps changing over time based on the performance of the players in the real world. With the new English Premier League season about to start, they have collected data from the past season and want to analyze it to determine the price of each player for the start of the new season.

You are a professional data scientist. Your task is to conduct a cluster analysis to identify players of different potentials based on previous season performances.

The objective is to understand the patterns in player performances and fantasy returns. This data analysis should facilitate the fantasy sports participant to better decide the exact price to be set for each player for the upcoming football season.

--------------------------
## **Data Description (Soccer/European Football)**
--------------------------

- **Player_Name:** Name of the player.
- **Club:** Club in which the player plays.
- **Position:** Position in which the player plays.
- **Goals_Scored:** Number of goals scored by the player in the previous season.
- **Assists:** Number of passes made by the player leading to goals in the previous season.
- **Total_Points:** Total number of fantasy points scored by the player in the previous season.
- **Minutes:** Number of minutes played by the player in the previous season.
- **Goals_Conceded:** Number of goals conceded by the player in the previous season.
- **Creativity:** A score, computed using a range of stats, that assesses player performance in terms of producing goalscoring opportunities for other players.
- **Influence:** A score, computed using a range of stats, that evaluates a player's impact on a match, taking into account actions that could directly or indirectly affect the match outcome.
- **Threat:** A score, computed using a range of stats, that gauges players who are most likely to score goals.
- **Bonus:** Total bonus points received. The three best performing players in each match receive additional bonus points based on a score computed using a range of stats. 3 points are awarded to the highest scoring player, 2 to the second best, and 1 to the third.
- **Clean_Sheets:** Number of matches without conceding a goal in the previous season.

## **Importing the necessary libraries and overview of the dataset**
"""

!pip install scikit-learn-extra;

# Commented out IPython magic to ensure Python compatibility.
# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns

sns.set_theme(style='white')

# Sets or removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)

# Sets or removes the limit for the number of displayed rows
pd.set_option("display.max_rows", None)

# To scale the data using z-score
from sklearn.preprocessing import StandardScaler

# To compute distances
from scipy.spatial.distance import cdist, pdist

# To perform K-Means clustering and compute silhouette scores
from sklearn.cluster import KMeans

#To compute silhouette scores
from sklearn.metrics import silhouette_score

# To import K-Medoids
from sklearn_extra.cluster import KMedoids

# To import DBSCAN
from sklearn.cluster import DBSCAN

# To import Gaussian Mixture
from sklearn.mixture import GaussianMixture

# To perform hierarchical clustering, compute cophenetic correlation, and create dendrograms
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage, cophenet

# Import all available color libraries
import matplotlib.colors as mcolors
import colorsys
import matplotlib.cm as cm

import warnings
warnings.filterwarnings('ignore')

# %matplotlib inline

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Define path of the dataset
file_path = "/content/drive/MyDrive/2024_mit/Data Sets_Data Frames/fpl_data.csv"

# Name the original dataframe "data"
data = pd.read_csv(file_path)

"""---


**Inspect the DATA**
"""

# Get shape of the dataset
data.shape

"""The dataset has 476 rows and 13 columns.

The data.head() function in Python, particularly when working with pandas DataFrames, is used to display the first few rows of the dataset. By default, data.head() shows the first 5 rows, but you can specify the number of rows you want to see by passing an argument, like data.head(10) to see the first 10 rows.

The purpose of data.head() is to give you a quick overview of the dataset, including its structure, the data types, and the content of the first few entries, which can be helpful for initial data inspection and exploration.
"""

data.head(10)

"""Using `data.sample(10, random_state=1)` in pandas is a way to randomly select a subset of rows from your DataFrame. Here’s a breakdown of its components and why you might use it:

- data.sample(10): This part specifies that you want to randomly select 10 rows from your DataFrame. It's useful when you want to get a quick sense of the data without looking at the entire dataset, especially if the dataset is large.

- random_state=1: The `random_state` parameter is a seed for the random number generator. Setting `random_state=1` ensures that the sample is reproducible, meaning that every time you run the code, you will get the same 10 rows. This is particularly useful for debugging, sharing results with others, or for consistency in presentations or reports.

Random Sampling:
- This allows you to inspect different parts of your data without bias toward the first few or last few rows.
- Using `random_state` helps ensure that your results can be replicated exactly by others, or by you in the future.

In summary, `data.sample(10, random_state=1)` is used to obtain a random but reproducible subset of 10 rows from a DataFrame, which is helpful for data exploration and ensuring consistency in results.
"""

# View 10 random rows of the data
data.sample(10, random_state = 1)

"""**Observations:**

- The shape function has told us that the dataset has 476 rows and 13 columns.
- The head function shows us that the 13 features/columns have different datatypes.
- The structure of our data looks good. The data was loaded correctly.
"""

# Copy the data to another variable, "df" to avoid any changes to original data
df = data.copy()

"""

---


The df.info() function in pandas provides a concise summary of a DataFrame, which is what you are seeing in the output. The "#" column is temporary and is not is the actual dataset."""

# Checking datatypes and number of non-null values for each column
data.info()

"""Summary of the DataFrame:

Class Information: The first line tells you that the data structure is a pandas.core.frame.DataFrame.
RangeIndex: The DataFrame has 476 entries, indexed from 0 to 475. This means there are 476 rows in the DataFrame.

Column Information:
- There are a total of 13 columns in the DataFrame.
- Column names are: Player_Name, Club, Position, etc.
- Non-null Count: The number of non-null (i.e., non-missing) entries in each column.
- *For* all columns in this dataset, there are 476 non-null entries, which means there are no missing values.
- Dtype: The data type of each column:
- object: Typically used for string or mixed types.
- int64: 64-bit integer.
- float64: 64-bit floating-point.
- Memory Usage: The amount of memory the DataFrame is using, which is 48.5+ KB in this case.

Why is df.info() Useful?
- Checking for Missing Data: By looking at the non-null counts, you can quickly identify if there are any missing values in your dataset.
- Understanding Data Types: Knowing the data types of each column helps you understand how the data is stored and whether any type conversions might be needed for analysis.
- Memory Usage: Helps in understanding the memory footprint of your DataFrame, which can be important when working with large datasets.

This summary is particularly useful when you’re starting to explore a new dataset, as it provides a quick overview of the structure, data types, and completeness of the data.

Important Observations for analysis:
- `Player_Name`, `Club`, and `Position` are categorical variables.
- All the other columns in the data are numeric in nature.
"""

# Check for duplicate values
data.duplicated().sum()

"""Observation:
- There are no duplicate values in the data.
"""

# Check for missing values (although we know this dataset has no missing values)
data.isnull().sum()

"""Observation:
- There are no missing values in the data.

## **Exploratory Data Analysis**
"""

# Check the statistical summary of the data
data.describe(include = 'all').T

"""Based on the statistical summary provided by `data.describe(include='all').T`, we can make several observations about the dataset:

### 1. **Categorical Data:**
   - **Player_Name:**
     - There are 476 unique player names, which implies that each player in the dataset is unique.
   - **Club:**
     - There are 17 unique clubs, with "Arsenal" appearing most frequently (30 times).
   - **Position:**
     - There are 4 unique positions, with "Midfielder" being the most common (195 occurrences).

### 2. **Numerical Data:**
   - **Goals_Scored:**
     - Mean (average) number of goals scored is approximately 1.91, with a standard deviation of 3.46.
     - The range is from 0 to 23 goals, indicating a wide variation in scoring among players.
   - **Assists:**
     - Mean number of assists is approximately 1.75, with a standard deviation of 2.71.
     - The range is from 0 to 14 assists.
   - **Total_Points:**
     - The mean total points is about 58.52, with a standard deviation of 51.29.
     - The range is from 0 to 244 points, indicating some players contribute significantly more than others.
   - **Minutes:**
     - Players have an average of 1336.91 minutes played, with a high standard deviation of 1073.77.
     - The range is from 0 to 3420 minutes, showing that some players have played extensively while others have played very little or not at all.
   - **Goals_Conceded:**
     - The mean is approximately 19.16 goals conceded, with a standard deviation of 15.95.
     - The range is from 0 to 68 goals conceded.
   - **Creativity, Influence, Threat:**
     - These are more specialized metrics:
       - Creativity has a mean of 195.98, with significant variability (std: 251.48).
       - Influence has a mean of 294.62, also with high variability (std: 267.78).
       - Threat has a mean of 224.96, with a standard deviation of 318.24.
     - These metrics likely capture more nuanced aspects of player performance.
   - **Bonus:**
     - Mean bonus points is 4.71, with a range from 0 to 40.
   - **Clean_Sheets:**
     - The mean is 4.75, with a standard deviation of 4.39.
     - The range is from 0 to 19 clean sheets.

### 3. **Overall Observations:**
   - **Variation in Performance:**
     - There is significant variability across players in terms of goals scored, assists, total points, and minutes played.
   - **Positional Impact:**
     - The number of Midfielders is much higher than other positions, which might suggest more midfielders in the dataset or that midfielders are more frequently substituted, leading to more entries.
   - **Specialized Metrics:**
     - Metrics like Creativity, Influence, and Threat have high standard deviations, indicating that these traits vary greatly among players.

### 4. **Data Completeness:**
   - There are no missing values in any of the columns, as indicated by the non-null counts being equal to the total number of entries (476) for all columns.

These observations can help in understanding the distribution of player performance and the dataset's overall characteristics, which is useful for further analysis or model building.

---


### **Univariate Analysis**

**Univariate Analysis will allow us to better understand and visualize the data.**

Categorical Variables:
- Frequency Distribution: Count the occurrences of each category.
- Bar Charts/Pie Charts: Visualize the frequency of each category.
- Mode: Identify the most common category.

Numerical Variables:
- Summary Statistics: Calculate mean, median, mode, range, variance, standard deviation, etc.
- Histograms: Visualize the distribution of the data.
- Box Plots: Identify the spread of the data and detect outliers.
- Kurtosis and Skewness: Assess the shape of the distribution.

Outlier Detection:
- Use box plots or z-scores to identify potential outliers in your data.
Handling Missing Values:
- If applicable, analyze the distribution of missing data and consider strategies like imputation.

---


### **Categorical Data Visualization and Observations**
"""

def my_labeled_barplot(data, feature, perc=False, n=None, order=None, palette='viridis', ascending=False):
    """
    Creates a bar plot with labels showing counts or percentages, with sorting options.

    Args:
        data: The DataFrame containing the data.
        feature: The name of the categorical column to plot.
        perc: If True, display percentages on labels, otherwise counts.
        n: (Optional) Width of the plot figure.
        order: (Optional) 'asc' for ascending order, 'desc' for descending order,
               or None for no specific order.
        palette: (Optional) Color palette for the bars.
        ascending: (Optional) Used when 'order' is not specified. Sorts bars in
                   ascending order if True, descending if False.
    """
    plt.figure(figsize=(n if n else len(data.columns) + 1, 5))

    if order == 'asc':
        order_index = data[feature].value_counts(ascending=True).index
    elif order == 'desc':
        order_index = data[feature].value_counts(ascending=False).index
    else:
        order_index = data[feature].value_counts(ascending=not ascending).index # Sort based on ascending parameter

    ax = sns.countplot(data=data, x=feature, order=order_index, palette=palette)

    if perc:
        total = len(data[feature])
        for p in ax.patches:
            percentage = '{:.1f}%'.format(100 * p.get_height() / total)
            x = p.get_x() + p.get_width() / 2 - 0.1
            y = p.get_y() + p.get_height()
            ax.annotate(percentage, (x, y), size=10)
    else:
        for p in ax.patches:
            x = p.get_x() + p.get_width() / 2 - 0.1
            y = p.get_y() + p.get_height()
            ax.annotate(p.get_height(), (x, y), size=10)

    plt.xticks(rotation=90, fontsize=12)

# Example usage:
# my_labeled_barplot(data, 'Club', perc=True, order='asc')  # Ascending order
# my_labeled_barplot(data, 'Club', perc=True, order='desc') # Descending order
# my_labeled_barplot(data, 'Club', perc=True, ascending=False) # Descending order without using 'order' parameter

my_labeled_barplot(data, 'Club', perc=True, order='asc')  # Ascending order

"""Observations:
- All of the clubs have a similar number of players per club, as demonstrated by their similar counts and percentages.
"""

# Create a pie chart of the clubs accompanied by a legend with total number of points

# Calculate total points for each club
club_points = data.groupby('Club')['Total_Points'].sum().sort_values(ascending=False)

# Create a color palette
colors = plt.cm.tab20(np.linspace(0, 1, len(club_points)))

# Create a pie chart
plt.figure(figsize=(10, 10))
plt.pie(club_points, labels=club_points.index, autopct='%1.1f%%', colors=colors, startangle=90)
plt.title('Total Points Distribution by Club', color='purple', fontsize=15)

# Create a legend with total points
legend_labels = [f"{club} ({points})" for club, points in club_points.items()]
plt.legend(legend_labels, loc='center left', bbox_to_anchor=(1.2, .65))

"""Observations:


*   Manchester City scored the most points, 7.6%
*   Newcastle United scored the least points, 4.8%
*  Aston Villa, Leeds United and West Ham United scored the same% amount of points, 6.1%
  *   However, they scored a slightly different number of total points
    *   Aston Villa, total points = 1706
    *   Leeds United, total points = 1691
    *   West Ham United, total points = 1686
"""

import plotly.express as px

# Create a DataFrame with only the desired clubs
selected_clubs = data[data['Club'].isin(['Aston Villa', 'Liverpool', 'Manchester United', 'Tottenham Hotspurs'])]

# Create the scatter plot
fig = px.scatter(selected_clubs, x='Player_Name', y='Total_Points', color='Club',
                 color_discrete_map={'Aston Villa': 'red', 'Liverpool': 'yellow',
                                     "Manchester United": 'royalblue', 'Tottenham Hotspurs': 'springgreen'},
                 title='Total Points by Players from Selected Clubs')

# Rotate x-axis labels for better readability
fig.update_layout(xaxis_tickangle=-45)

"""**Use the above interactive plot to see the names of players in relation to their total points scored and club.**"""

# Create a pie chart of the positions accompanied by a legend with total number of points and annotate with the total points by position

# Calculate total points for each club
position_points = data.groupby('Position')['Total_Points'].sum().sort_values(ascending=False)

# Create a color palette
colors = ['lavender', 'gold', 'darkorchid', 'yellow']

# Create a pie chart
plt.figure(figsize=(10, 10))
plt.pie(position_points, labels=position_points.index, colors=colors, startangle=90, autopct='%1.1f%%')
plt.title('Total Points Distribution by Position', color='purple', fontsize=15)
# Create a legend with total points
legend_labels = [f"{position} ({points})" for position, points in position_points.items()]
plt.legend(legend_labels, loc='center left', bbox_to_anchor=(1.2, .65), title='Total Points')

"""Observations:
- The Midfielder and the Defender scored the most points, total points = 12,276
- The Goalkeeper scored the least points, total points = 2,481
- The number of midfielders in the data is more than four times the number of goalkeepers.
  - The reason for this is that a team can only play one goalkeeper in a match.
- The number of defenders in the data is nearly 3 times the number of forwards.
  - This is a team formation trend.
  - Most teams tend to have 1 or 2 forwards.
"""

# Create a DataFrame with only the desired clubs
selected_positions = data[data['Position'].isin(['Midfielder', 'Defender', 'Goalkeeper', 'Forward'])]

# Create the scatter plot
fig = px.scatter(selected_positions, x='Position', y='Total_Points', color='Position',
color_discrete_map={'Midfielder': 'gold', 'Defender': 'darkorchid',
                                     'Goalkeeper': 'yellow', 'Forward': 'blueviolet'},
                 title='Total Points by Players from Selected Positions',
                 hover_name='Player_Name')

# Rotate x-axis labels for better readability
fig.update_layout(xaxis_tickangle=-45)

"""**Use the above interactive plot to see the names of players in relation to their total points scored and position.**


"""

# Frequency distribution
position_counts = data['Position'].value_counts()
print(position_counts)

"""**Numerical (Quantitative) Data Visualization and Observations**"""

# Define a function that creates a boxplot and a histogram for a feature
def histogram_boxplot_fun(data, feature, kde = False, bins = 30):
    fig, ax = plt.subplots(2,1, sharex=True, gridspec_kw={"height_ratios": (0.25, 0.75)}, figsize = (12, 7))
    sns.boxplot(data, x = feature, showmeans = True, color = "cyan", ax = ax[0])
    sns.histplot(data, x = feature, kde = True, bins = 23, color = "lime", ax = ax[1])
    ax[1].axvline(df[feature].mean(), color = 'purple', linestyle = '--')
    ax[1].axvline(df[feature].median(), color = 'yellow', linestyle = '-')
    plt.tight_layout()

histogram_boxplot_fun(data, 'Total_Points')

data[data['Total_Points'] >= 212]

"""Observations:
- The histogram shows a right-skewed distribution. The majority of players have lower total points, with a significant number concentrated below 50 points. The frequency decreases as the total points increase, indicating that fewer players achieve high total points.
- The mean is slightly higher than the median, reinforcing the right-skewness of the distribution.
- The boxplot shows four outliers on the right-hand side, indicating that a small number of players have exceptionally high total points compared to the rest. These outliers stretch the distribution and contribute to the skewness.
- The box, representing the interquartile range (IQR), is relatively wide, suggesting a significant spread in the middle 50% of the data. The spread indicates diversity in player performance, with a range of total points among the majority of players.
- The whiskers extend fairly far, indicating that while there are outliers, the majority of the data is within a reasonable range.
- The histogram's highest bar is at the lower end, around 0–50 total points.
  - The upper whisker outliers are: Mohamed Salah, Bruno Fernandes, Harry Kane, Heung-Min Son.
"""

histogram_boxplot_fun(data, 'Goals_Scored')

"""Observations:
- The distribution is right-skewed and few players have scored more than 15 goals.
- Very few players have scored more than 10 goals.
- There are 15 outliers who score significantly more points than the mean and the median.
- Median of Goals Scored: 0.5
- Mean of Goals Scored: 1.91
- The Goals Scored outliers who scored above the mean are:
  Youri Tielemans, Wilfried Zaha, Tomas Soucek, Timo Werner, Tammy Abraham, Stuart Dallas, Sadio Mane, Rodrigo Moreno, Roberto Firmino, Riyad Mahrez, Richarlison de Andrade, Raphael Dias Belloli, Raheem Sterling, Pierre-Emerick Aubameyang, Phil Foden, Patrick Bamford, Ollie Watkins, Nicolas Pepe, Neal Maupay, Mohamed Salah, Michail Antonio, Mason Mount, Mason Greenwood, Marcus Rashford, Kevin De Bruyne, Kelechi Iheanacho, Joseph Willock, Jorge Luiz Frello Filho, Jesse Lingard, Jarrod Bowen, Jamie Vardy, James Ward-Prowse, James Rodriguez, James Maddison, Jack Harrison, Jack Grealish, Ilkay Gundogan, Heung-Min Son, Harvey Barnes, Harry Kane, Gylfi Sigurdsson, Gareth Bale, Gabriel Fernando de Jesus, Ferran Torres, Edinson Cavani, Dominic Calvert-Lewin, Diogo Jota, Danny Welbeck, Danny Ings, Christian Benteke, Chris Wood, Che Adams, Callum Wilson, Bruno Fernandes, Bertrand Traore, Anwar El Ghazi, Alexandre Lacazette.


"""

# Print the median and print the mean of goals scores to no more than two decimal places
print('Median of Goals Scored: {:.2f}'.format(data['Goals_Scored'].median()))
print('Mean of Goals Scored: {:.2f}'.format(data['Goals_Scored'].mean()))

# Define one tailed outliers using IQR who score above the mean
Q1_GS = data['Goals_Scored'].quantile(0.25)
Q3_GS = data['Goals_Scored'].quantile(0.75)
IQR_GS = Q3_GS - Q1_GS
upper_whisker_GS = Q3_GS + 1.5 * IQR_GS
outliers_Goals_Scored = data[data['Goals_Scored'] > upper_whisker_GS]['Player_Name'].sort_values(ascending=False)
print(outliers_Goals_Scored)

# Print the Goals_Scored outlier names in string format
outlier_list = outliers_Goals_Scored.tolist()
outlier_string = ", ".join(outlier_list)
print("The Goals_Scored outliers are: " + outlier_string)

histogram_boxplot_fun(df, 'Assists')

"""Observations:
- The distribution is right-skewed and few players have assisted more than 8 goals.
- Very few players have assisted more than 11 goals.
- Most of the players didn't assist any goals.
- There are 8 outliers who have significantly more assists than the mean and the median.
- The top 8 Assists outliers are: Willian Borges Da Silva, Vladimir Coufal, Trent Alexander-Arnold, Timo Werner, Stuart Armstrong, Sergio Reguilon, Said Benrahma, Sadio Mane.
- The Assists outliers are: Willian Borges Da Silva, Vladimir Coufal, Trent Alexander-Arnold, Timo Werner, Stuart Armstrong, Sergio Reguilon, Said Benrahma, Sadio Mane, Roberto Firmino, Riyad Mahrez, Richarlison de Andrade, Raphael Dias Belloli, Raheem Sterling, Phil Foden, Pedro Lomba Neto, Paul Pogba, Patrick Bamford, Pascal Gross, Pablo Fornals, Ollie Watkins, Mohamed Salah, Mason Mount, Marcus Rashford, Marc Albrighton, Lucas Digne, Leandro Trossard, Kevin De Bruyne, Kai Havertz, John McGinn, Jarrod Bowen, Jamie Vardy, James Ward-Prowse, James Maddison, Jack Harrison, Jack Grealish, Heung-Min Son, Harry Kane, Eberechi Eze, Dominic Calvert-Lewin, Che Adams, Callum Wilson, Bruno Fernandes, Bertrand Traore, Bernardo Silva, Benjamin Chilwell, Anthony Martial, Andrew Robertson, Aaron Cresswell.
"""

# Define one tailed outliers using IQR who score above the mean
Q1_A = data['Assists'].quantile(0.25)
Q3_A = data['Assists'].quantile(0.75)
IQR_A = Q3_A - Q1_A
upper_whisker_A = Q3_A + 1.5 * IQR_A
outliers_Assists = data[data['Assists'] > upper_whisker_A]['Player_Name'].sort_values(ascending=False)
print(outliers_Assists)

# Print the Assists outlier names in string format
outlier_list = outliers_Assists.tolist()
outlier_string = ", ".join(outlier_list)
print("The Assists outliers are: " + outlier_string)

# Print the top 8 outliers in feature Assists in string format
top_8_outliers = outliers_Assists.head(8).tolist()
top_8_outlier_string = ", ".join(top_8_outliers)
print("The top 8 Assists outliers are: " + top_8_outlier_string)

histogram_boxplot_fun(df, 'Minutes')

"""Observations:
- The box represents the IQR (the range between the 25th and 75th percentiles), and it appears to be quite wide, indicating significant spread in the middle 50% of the data.
- The whiskers extend quite far from the box, suggesting that there is a broad range of values in the dataset.
- A lot of players didn't have any field time at all
- The histogram shows a distribution that is right-skewed, with a higher frequency of lower "Minutes" values and a tail extending towards the higher "Minutes" values.
- The histogram appears to be unimodal (one peak), with a prominent peak at the lowest interval (near 0 minutes), indicating that many observations are clustered around this value.
    - Total number of players who had 0 Minutes = 45
    - Total number of players = 476
    - % of total number of players who had 0 Minutes = 9.45%
- There seems to be a smaller second peak around the 2500 to 3000-minute range, which could indicate a subgroup in the data that behaves differently from the main group.
"""

# Print "Total number of players who had 0 Minutes ="
# Print "Total number of players ="
# Print "% of total number of players who had 0 Minutes ="

# Calculate the total number of players who had 0 minutes
zero_minutes_players = data[data['Minutes'] == 0]['Player_Name'].count()

# Calculate the total number of players
total_players = data['Player_Name'].count()

# Calculate the percentage of players who had 0 minutes
percentage_zero_minutes = (zero_minutes_players / total_players) * 100

# Print the results
print("Total number of players who had 0 Minutes =", zero_minutes_players)
print("Total number of players =", total_players)
print("% of total number of players who had 0 Minutes = {:.2f}%".format(percentage_zero_minutes))

histogram_boxplot_fun(data, 'Goals_Conceded')

"""Observations:
- The mean is slightly right of center within the box, suggesting a right-skewed distribution.
- The box indicates the IQR, with a relatively balanced spread around the median. This suggests that the middle 50% of the data is somewhat evenly distributed.
- The whiskers extend outward, with the right whisker being significantly longer, indicating that the distribution has a broader range on the higher end of goals conceded.
- The mean, represented by the green triangle, is located closer to the center of the box, which suggests a more symmetric distribution of “Goals_Conceded” around the median.
- The histogram appears to be unimodal, with the most frequent range of goals conceded being between 0 and 10.
- There is a large concentration of observations(players) that have conceded a low number of goals.
- The right tail of the histogram extends, showing that while fewer teams concede a large number of goals, there is still a considerable range in the data, going up to around 70 goals conceded.

**These observations suggest that most players in the dataset have conceded a relatively low number of goals, with a few players that have conceded significantly more, leading to a right-skewed distribution. The presence of a mild outlier indicates some variability within the dataset, though it does not significantly deviate from the overall pattern.**


"""

# Create a pie chart that shows goals conceded by clubs
# Calculate total goals conceded for each club
club_conceded = data.groupby('Club')['Goals_Conceded'].sum().sort_values(ascending=False)

# Create a pie chart
plt.figure(figsize=(10, 10))
plt.pie(club_conceded, labels=club_conceded.index, autopct='%1.1f%%', colors=colors, startangle=90)
plt.title('Total Goals Conceded Distribution by Club', color='purple', fontsize=15)

# Create a legend with total points
legend_labels = [f"{club} ({points})" for club, points in club_conceded.items()]
plt.legend(legend_labels, loc='center left', bbox_to_anchor=(1.2, .65))

# Create a pie chart that shows goals conceded by Clubs
# Create a subplot that shows this pie chart alongside the total points by Clubs

# Calculate total points for each club
club_points = data.groupby('Club')['Total_Points'].sum().sort_values(ascending=False)

# Calculate total goals conceded for each club
club_conceded = data.groupby('Club')['Goals_Conceded'].sum().sort_values(ascending=False)

# Create a color palette with a fixed order based on club_points
colors = plt.cm.tab20(np.linspace(0, 1, len(club_points)))
color_dict = {club: color for club, color in zip(club_points.index, colors)}

# Create subplots
fig, axes = plt.subplots(1, 2, figsize=(20, 10))

# Pie chart for total points
axes[0].pie(club_points, labels=club_points.index, autopct='%1.1f%%', colors=[color_dict[club] for club in club_points.index], startangle=90, textprops={'fontsize': 6})
axes[0].set_title('Total Points Distribution by Club', color='purple', fontsize=15)
legend_labels_points = [f"{club} ({points})" for club, points in club_points.items()]
axes[0].legend(legend_labels_points, loc='center left', bbox_to_anchor=(1.2, .65))

# Pie chart for total goals conceded
axes[1].pie(club_conceded, labels=club_conceded.index, autopct='%1.1f%%', colors=[color_dict[club] for club in club_conceded.index], startangle=90, textprops={'fontsize': 6})
axes[1].set_title('Total Goals Conceded Distribution by Club', color='purple', fontsize=15)
legend_labels_conceded = [f"{club} ({points})" for club, points in club_conceded.items()]
axes[1].legend(legend_labels_conceded, loc='center left', bbox_to_anchor=(1.2, .65))

plt.tight_layout()

"""Observations:
- There appears to be a correlation between the number of points a club has and the number of goals they conceded.
  - Manchester City and Chelsea are both among the top in points and have conceded fewer goals, indicating a strong overall performance.
- Defensive Impact: Clubs with high goals conceded, like Southampton and Crystal Palace, also have lower points, which suggests that defensive weaknesses significantly impacted their overall performance in the league.
- These visualizations effectively highlight the relationship between a club’s defensive performance (in terms of goals conceded) and their success in terms of total points accumulated.

**The stronger the defense, the better the club seems to perform in the league standings.**

Total Points Distribution by Club:
- Top Performers:
  - Manchester City leads with the highest points at 6.7%, indicating that it is the top-performing club in terms of points.
  - Chelsea and Liverpool follow closely with 6.3% and 6.1%, respectively, showing that they are also among the top clubs.

- Mid-Tier Clubs:
  - Manchester United (6.0%) and Tottenham Hotspurs (6.0%) are just slightly behind the top three, indicating competitive performance.
  - Aston Villa, Leeds United, and West Ham United are mid-tier, with their points ranging from around 5.5% to 5.8%.
- Lower Performers:
  - Newcastle United and Crystal Palace are among the clubs with the lowest points (3.7% and 3.9%, respectively), indicating they are underperforming compared to the others.

Total Goals Conceded Distribution by Club:
- High Goals Conceded:
  - Southampton (8.0%) and Crystal Palace (7.2%) have the highest percentages of goals conceded, suggesting defensive weaknesses.
  - Newcastle United (6.9%) and Burnley (6.9%) also conceded a high number of goals, indicating they may have struggled defensively throughout the season.
- Moderate Goals Conceded:
  - Leeds United (5.9%) and Wolverhampton Wanderers (5.5%) fall into a mid-range category, indicating that they have a moderately strong defense.
- Low Goals Conceded:
  - Chelsea (3.9%) and Manchester City (3.8%) have the lowest percentages of goals conceded, highlighting strong defensive performances.
  - Arsenal (4.5%) and Liverpool (4.6%) also have relatively low goals conceded, suggesting effective defensive strategies.
"""

histogram_boxplot_fun(data, 'Creativity')

"""Observations:
- The mean, represented by the green triangle in the boxplot, is located towards the lower end of the x-axis. This suggests that the distribution of "Creativity" values is right-skewed, with a majority of the data points clustered towards lower values.
- The box, which represents the IQR, is relatively compact, indicating that the middle 50% of the "Creativity" values are closely grouped together.
- The right whisker extends further out, and there are numerous outliers beyond the whisker, indicating that there are several "Creativity" values significantly higher than the rest of the data.
- These outliers stretch far beyond the IQR, suggesting that a small number of observations have very high creativity scores.
- The histogram is heavily right-skewed, with the majority of "Creativity" values concentrated at the lower end of the scale (near 0). This indicates that most players have low creativity scores.
- The long tail on the histogram shows that while most entities have low creativity scores, a few have exceptionally high scores, stretching up to around 1400.
  - The Creativity outliers are: Youri Tielemans, Trent Alexander-Arnold, Sadio Mane, Raphael Dias Belloli, Pedro Lomba Neto, Pascal Gross, Mohamed Salah, Mason Mount, Luke Shaw, Kevin De Bruyne, James Ward-Prowse, James Maddison, Jack Harrison, Jack Grealish, Heung-Min Son, Dwight McNeil, Bruno Fernandes, Ashley Westwood, Andrew Robertson, Adama Traore, Aaron Cresswell.

**Most players exhibit low creativity, with a select few showing very high levels.**
"""

# Create a subplot with two pie charts, chart 1 should be the points scored and chart 2 should be creativity by club
# Calculate total points for each club
club_points = data.groupby('Club')['Total_Points'].sum().sort_values(ascending=False)

# Calculate total creativity for each club
club_creativity = data.groupby('Club')['Creativity'].sum().sort_values(ascending=False)

# Create a color palette with a fixed order based on club_points
colors = plt.cm.tab20(np.linspace(0, 1, len(club_points)))
color_dict = {club: color for club, color in zip(club_points.index, colors)}

# Create subplots
fig, axes = plt.subplots(1, 2, figsize=(20, 10))

# Pie chart for total points
axes[0].pie(club_points, labels=club_points.index, autopct='%1.1f%%', colors=[color_dict[club] for club in club_points.index], startangle=90, textprops={'fontsize': 6})
axes[0].set_title('Total Points Distribution by Club', color='purple', fontsize=15)
legend_labels_points = [f"{club} ({points})" for club, points in club_points.items()]
axes[0].legend(legend_labels_points, loc='center left', bbox_to_anchor=(1.2, .65))

# Pie chart for total creativity
axes[1].pie(club_creativity, labels=club_creativity.index, autopct='%1.1f%%', colors=[color_dict[club] for club in club_creativity.index], startangle=90, textprops={'fontsize': 6})
axes[1].set_title('Total Creativity Distribution by Club', color='purple', fontsize=15)
legend_labels_creativity = [f"{club} ({points})" for club, points in club_creativity.items()]
axes[1].legend(legend_labels_creativity, loc='center left', bbox_to_anchor=(1.2, .65))

plt.tight_layout()

"""Observation:
- Manchester City again leads in creativity with 7.1%, indicating that their high points are likely due to creative play.
- Liverpool (7.1%) and Chelsea (7.6%) also show high creativity, which aligns with their strong point performances.
- Manchester United and Tottenham Hotspur both hold 6.9%, reflecting their significant creative contributions.

- There is a noticeable correlation between high creativity and high points; teams like Manchester City, Liverpool, and Chelsea rank high in both metrics, suggesting that creativity is a strong factor in their success.
- Aston Villa (5.1%) and Leeds United (5.3%) also maintain consistency in both points and creativity, indicating balanced performance.
- Some clubs like Burnley (4.3%) and Southampton (4.9%) show lower creativity compared to their points, suggesting that these teams may rely more on other strategies rather than creativity.
- Newcastle United (3.7% points, 4.7% creativity) and Crystal Palace (3.9% points, 4.3% creativity) show that while they may be more creative than their points suggest, this creativity has not translated into a proportional increase in points.
  - The Creativity outliers are: Youri Tielemans, Trent Alexander-Arnold, Sadio Mane, Raphael Dias Belloli, Pedro Lomba Neto, Pascal Gross, Mohamed Salah, Mason Mount, Luke Shaw, Kevin De Bruyne, James Ward-Prowse, James Maddison, Jack Harrison, Jack Grealish, Heung-Min Son, Dwight McNeil, Bruno Fernandes, Ashley Westwood, Andrew Robertson, Adama Traore, Aaron Cresswell


"""

# Print the names of the Creativity one-tailed outlier players as a string
Q1_C = data['Creativity'].quantile(0.25)
Q3_C = data['Creativity'].quantile(0.75)
IQR_C = Q3_C - Q1_C
upper_whisker = Q3_C + 1.5 * IQR_C
outliers_Creativity = data[data['Creativity'] > upper_whisker]['Player_Name'].sort_values(ascending=False)
outlier_list = outliers_Creativity.tolist()
outlier_string = ", ".join(outlier_list)
print("The Creativity outliers are: " + outlier_string)

histogram_boxplot_fun(data, 'Influence')

"""Observations:
- The median, indicated by the green triangle, is positioned slightly to the right within the box which is heavily weighted to the left, suggesting a right skew in the distribution of "Influence" values.
- The box, representing the IQR, is relatively wide, indicating that the middle 50% of the "Influence" values are moderately spread out.
- The right whisker extends further than the left, and there are two distinct outliers beyond the right whisker, indicating that there are a few "Influence" values significantly higher than the majority of the data.
- The histogram shows a slight right-skew, with a large number of observations clustered at the lower end of the "Influence" scale, though the skew is less pronounced than in the "Creativity" feature.
- The highest frequency is observed at the lowest "Influence" range (0-100), indicating that many players have low influence scores. The frequency gradually decreases as the "Influence" score increases, with a noticeable drop-off after around 400.
- The right tail extends out to around 1200, with some values beyond this range. This long tail is indicative of the outliers, as seen in the boxplot.
- The median (purple line) is slightly higher than the mean (yellow line), confirming the slight right skew in the distribution. The difference between the mean and median is not as large as in the "Creativity" feature, suggesting a more moderate skew.
- The moderate difference between the mean and median suggests that the data is somewhat symmetrically distributed but still leans towards higher values due to the influence of the outliers.
  - The Influence outliers with exceptional influence are: Harry Kane and Bruno Fernandes

**Observations for Clustering:**
- The visual representation of data indicates that we might be observing 3 to 5 natural clusters forming
"""

# Print the names of the Influence outlier players as a string
Q1_I = data['Influence'].quantile(0.25)
Q3_I = data['Influence'].quantile(0.75)
IQR_I = Q3_I - Q1_I
upper_whisker = Q3_I + 1.5 * IQR_I
outliers_Influence = data[data['Influence'] > upper_whisker]['Player_Name'].sort_values(ascending=False)
outlier_list_influence = outliers_Influence.tolist()
outlier_string_influence = ", ".join(outlier_list_influence)
print("The Influence outliers are: " + outlier_string_influence)

histogram_boxplot_fun(data, 'Threat')

"""Observations
- A threat score is a metric that measures the likelihood of an action resulting in a goal. Threat scores can be used by coaches, analysts, and managers to evaluate the effectiveness of their strategies, analyze past actions, and predict the success of future ones. They can also be used to evaluate a team's performance, as well as opposing players and teams.
- The green triangle representing the mean is located towards the left side of the x-axis, indicating a right-skewed distribution for the "Threat" feature.
- The box, which represents the IQR, is wide, showing a significant spread in the middle 50% of the data.
- The skewness is evident as the median is not centrally located within the box.
- The right whisker is long and there are numerous outliers beyond it, indicating a large number of observations with significantly higher "Threat" values compared to the bulk of the data.
- The histogram shows a heavily right-skewed distribution, with a large concentration of observations at the lower end of the "Threat" scale. Most of the data points fall within the 0-100 range.
- The highest frequency of "Threat" values is at the lower end, specifically between 0 and 50. This indicates that most players have low threat scores.
- The distribution has a long right tail, with some observations reaching up to nearly 2000, consistent with the outliers seen in the boxplot. This tail indicates that while most players have low threat values, there are some with very high threat scores.
- The mean (purple line) is higher than the median (yellow line), confirming the right skew. The mean is pulled up by the high "Threat" values of the outliers.
    - The Threat outliers are: Wilfried Zaha, Tomas Soucek, Timo Werner, Sadio Mane, Roberto Firmino, Riyad Mahrez, Richarlison de Andrade, Raphael Dias Belloli, Raheem Sterling, Pierre-Emerick Aubameyang, Patrick Bamford, Ollie Watkins, Nicolas Pepe, Neal Maupay, Mohamed Salah, Michail Antonio, Mason Mount, Mason Greenwood, Marcus Rashford, Leandro Trossard, Kevin De Bruyne, Kelechi Iheanacho, Jarrod Bowen, Jamie Vardy, Jack Grealish, Heung-Min Son, Harvey Barnes, Harry Kane, Gabriel Fernando de Jesus, Fabio Silva, Dominic Calvert-Lewin, Danny Ings, Christian Benteke, Chris Wood, Che Adams, Callum Wilson, Bukayo Saka, Bruno Fernandes, Bertrand Traore, Alexandre Lacazette

**Observations for Clustering:**
- We might want to drop this feauture when conducting our cluster analysis, as this feature aligns more with the Club's overall performance than the player's performance.

"""

# Print the names of the Threat outlier players as a string use a new variable
Q1_T = data['Threat'].quantile(0.25)
Q3_T = data['Threat'].quantile(0.75)
IQR_T = Q3_T - Q1_T
upper_whisker_T = Q3_T + 1.5 * IQR_T
outliers_Threat = data[data['Threat'] > upper_whisker_T]['Player_Name'].sort_values(ascending=False)
outlier_list_threat = outliers_Threat.tolist()
outlier_string_threat = ", ".join(outlier_list_threat)
print("The Threat outliers are: " + outlier_string_threat)

histogram_boxplot_fun(data, 'Bonus')

"""Observations:
- Bonus is the total bonus points received. The three best performing players in each match receive additional bonus points based on a score computed using a range of stats. 3 points are awarded to the highest scoring player, 2 to the 2nd best, and 1 to the 3rd.

- The green triangle representing the mean is located to the right of the median (yellow line) within as plotted on the shared x-axis. This suggests a right-skewed distribution, where a few higher values are pulling the mean upwards, but the majority of the data is concentrated toward the lower values.
- The box, representing the IQR, shows that the middle 50% of "Bonus" values are spread out, but they are more concentrated toward the lower end of the range.
- The right whisker extends very far, and there are several outliers visible beyond the whisker. These outliers represent observations with significantly higher bonus values compared to the rest of the data, contributing to the overall right skewness.
- The histogram clearly shows a right-skewed distribution, with the majority of "Bonus" values clustered at the lower end (0-5). This aligns with the boxplot's indication of right skewness.
- The highest frequency of "Bonus" values is observed at the lower end (0-2), showing that most players received few bonus points.
- The long tail extends to the right, up to around 35-40, indicating that while most players received few bonus points, a smaller group received significantly higher bonus values, which is consistent with the outliers seen in the boxplot.

  - The Bonus outliers are: Trent Alexander-Arnold, Richarlison de Andrade, Patrick Bamford, Ollie Watkins, Nick Pope, Mohamed Salah, Michail Antonio, Luke Shaw, Lucas Digne, Kevin De Bruyne, Kelechi Iheanacho, Joao Cancelo, Jamie Vardy, Illan Meslier, Ilkay Gundogan, Heung-Min Son, Harry Kane, Emiliano Martinez, Dominic Calvert-Lewin, Danny Ings, Callum Wilson, Bruno Fernandes, Andrew Robertson, Alexandre Lacazette, Aaron Cresswell.

**Observations for Clustering:**
  - Bonus and total points should be very closely correlated. We should consider dropping the feature "Bonus" when conducting a clustering analysis, in order to reduce the total number of features
  - The high kertosis as demonstrated by the extreme outliers will also effect clustering and so this feauture should be dropped when we conduct the cluster analysis
"""

# Print the names of the Bonus outlier players as a string
Q1_B = data['Bonus'].quantile(0.25)
Q3_B = data['Bonus'].quantile(0.75)
IQR_B = Q3_B - Q1_B
upper_whisker_B = Q3_B + 1.5 * IQR_B
outliers_Bonus = data[data['Bonus'] > upper_whisker_B]['Player_Name'].sort_values(ascending=False)
outlier_list_bonus = outliers_Bonus.tolist()
outlier_string_bonus = ", ".join(outlier_list_bonus)
print("The Bonus outliers are: " + outlier_string_bonus)

histogram_boxplot_fun(df, 'Clean_Sheets')

"""Observations:
- Clean_Sheets are the number of matches without conceding a goal in the previous season.
- The distribution of "Clean Sheets" is slightly right-skewed, meaning that the majority of the data is concentrated toward lower values, with some higher values pulling the mean to the right.
- The box, representing the IQR, shows that the middle 50% of "Clean Sheets" values are somewhat spread out but are skewed towards the lower end of the range.
- The median is closer to the lower quartile, indicating that there are more lower values in the data.
- The whiskers extend without showing any significant outliers, suggesting that most of the data falls within a reasonable range relative to the IQR.
- The histogram confirms a slight right-skew, with the highest frequency of "Clean Sheets" values observed at the lower end (around 0-2). The distribution tapers off as the number of clean sheets increases, indicating fewer observations at higher values.
- The highest bar in the histogram is at the very low end, specifically around 0, indicating that many players have very few clean sheets.
- The histogram shows that while there is a concentration of lower values, there is a reasonable spread across the middle range (5-10 clean sheets), with the frequency gradually decreasing as the number of clean sheets increases.
- The median (purple line) is slightly higher than the median (yellow line), which is consistent with the right skew observed in both the boxplot and histogram.
- The spread of data across the middle range suggests a diversity in performance, with a mix of players achieving varying numbers of clean sheets.
- The lack of significant outliers indicates that the distribution is relatively consistent without extreme deviations, which could suggest a more normal performance distribution across the dataset.
- These observations indicate that while there is a slight right skew in the "Clean Sheets" distribution, the data is fairly spread out across the middle range, with no extreme outliers, suggesting a relatively balanced distribution with a lean toward lower values.

**Observations for clustering:**
  - We continue to see about 4 natural clusters forming.

"""

# Number of Clean_Sheet outliers
Q1_CS = data['Clean_Sheets'].quantile(0.25)
Q3_CS = data['Clean_Sheets'].quantile(0.75)
IQR_CS = Q3_CS - Q1_CS
upper_whisker_CS = Q3_CS + 1.5 * IQR_CS
outliers_Clean_Sheets = data[data['Clean_Sheets'] > upper_whisker_CS]['Player_Name']
num_outliers = outliers_Clean_Sheets.shape[0]
print("Number of Clean_Sheets outliers:", num_outliers)

"""### **Bivariate Analysis**

We are going to conduct a birvariate analysis on the numerical features. In order to do this, we are going to drop the categorical features (Player_Name, Club and Position). The numerical features are Total_Points, Goals_Scored, Assists, Minutes, Goals_Conceded, Creativity, Influence, Threat, Bonus and Clean_Sheets).

Remember that the original dataset is the variable "data".
The copy of the dataset is "df".

All dropping of features and rows, and any other manipulation of the dataset should be executed on "df".
"""

# Identify categorical columns
categorical_columns = df.select_dtypes(include=['object', 'category']).columns

# Drop categorical columns
df = df.drop(columns=categorical_columns)

# Display the remaining DataFrame
(df.head(0))

# Calculate the correlation matrix
corr = df.corr()

# Create a heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(corr, annot=True, cmap='winter', fmt=".2f", linewidths=.5)
plt.title('Correlation Heatmap of Numerical Features', color='deepskyblue', fontsize=15)

"""Observations:
- The strongest correlation is between Total Points and Influence **0.96**, indicating that as the number of points increases, the influence score also tends to increase.
- There is a very strong positive correlation of **0.94** between Minutes and Influence.
- Also the correlation between Minutes and Goals Conceded is **0.94**. However more minutes on the field does not strongly correlated to more goals.
- There is a strong correlation of **0.83** between Creativity and Assists, which is expected as creative players are likely to set up goal-scoring opportunities.
- Threat and Goals Scored: The correlation is **0.90**, indicating that players who pose a higher threat are indeed more likely to score goals.
- Bonus and Total Points: There is a correlation of **0.87** between Bonus and Total Points, indicating that players who earn more bonus points also tend to have a higher overall point tally.
- Clean Sheets and Total Points: With a correlation of **0.89**, players that keep more clean sheets are likely to accumulate more points, reflecting the importance of a strong defense.
- Clean Sheets and Creativity: There’s a **0.62** correlation, which possibly indicates that creative play contributes to defensive stability (keeping clean sheets).
- Creativity and Total Points: The correlation here is **0.71**, showing that creativity contributes to a player’s overall point tally, but it’s not the sole factor.
- Creativity and Threat: The correlation is **0.66**, which is moderate, indicating that while creativity does contribute to being a threat, other factors are likely at play.
- It is surprising that the correlation between minutes an goals scored is only **0.44**, however than might be explained by more time given to defense players.
- It is also surprising that the correlation between total points and goals is **0.7** and not stronger.

**Observations for clustering:**
- High Influence, High Minutes, High Goals Conceded: Group players who are heavily involved in the game and play critical defensive roles. These players might be identified as key in high-stakes or challenging scenarios.
- Creativity and Assists: Group creative playmakers who are likely to assist in goal-scoring, driving offensive plays.
- Threat and Goals Scored: Group players who are consistent threats and primary scorers, which is crucial for identifying key offensive players.
- Defensive vs. Offensive Clusters: Separate players based on defensive metrics (Clean Sheets, Goals Conceded) vs. offensive metrics (Goals Scored, Assists, Creativity) to identify distinct roles and strategies.
- Considering Key Performance Metrics: Use correlations like Total Points with Bonus and Clean Sheets to form clusters of high-performing players who contribute across various metrics, highlighting well-rounded performance.

**Feature reduction:**
To reduce the number of features for clustering analysis while retaining the most informative ones, you should consider dropping features that are highly correlated with each other. If two features are strongly correlated (e.g., above 0.85; or 0.90), they provide similar information, so one of them can be dropped. Inversely, features with lower correlations that don’t contribute significantly to distinguishing between clusters might be also be removed.

**Suggested Features to Drop:**
1.  Assists:
  - Correlation with Creativity is high at 0.83, and with Total Points at 0.70.
  - Retain Creativity as it captures similar information but also has a broader impact on other features.
2.   Minutes:
  - Although Minutes are highly correlated with Influence (0.94) and Goals Conceded (0.94), Influence might be more insightful as it encapsulates the impact a player has during the time they are on the field. Hence, you could drop Minutes and retain Influence.
3.	Bonus:
  - Bonus has a high correlation with Total Points (0.87), so you could drop Bonus and keep Total Points, which encompasses the overall performance.
4. Clean Sheets:
  - Clean Sheets has a moderate correlation with Goals Conceded (0.76) and Total Points (0.89).
  - Since Goals Conceded and Total Points are more directly indicative of overall performance, Clean Sheets might be redundant and might be dropped.

**Features to Retain:**
1.   Goals Scored:
  - Directly measures offensive effectiveness and has significant correlations with several key features.
2.   Creativity:
  - Captures a player’s ability to generate scoring opportunities and is strongly linked to assists.
3. Total Points:
  - A composite measure that reflects overall performance, making it a crucial feature for clustering.
4. Influence:
  - Reflects a player’s impact on the game, strongly correlated with playing time and overall engagement.
5. Threat:
  - Directly related to scoring potential, with a strong correlation to Goals Scored.

**Option 1: Final List of Features for Clustering:**
1. Goals Scored
2.	Creativity
3.	Total Points
4.	Influence
5.	Threat
6.	Goals Conceded (if a defensive perspective is necessary)

By focusing on these features, you might likely retain a balance of offensive, creative, and defensive metrics while reducing redundancy, making your clustering analysis more efficient and meaningful.
"""

plt.figure(figsize=(15,6))
sns.scatterplot(x='Minutes', y='Total_Points', data=data, color="hotpink")
plt.title('Correlation between Minutes Played and Total Points', color="purple", fontsize=15)
plt.xlabel('Minutes Played')
plt.ylabel('Total Points')

# Add vertical lines for mean and median of Minutes
plt.axvline(data['Minutes'].mean(), color='blue', linestyle='--', label='Mean Minutes')
plt.axvline(data['Minutes'].median(), color='green', linestyle='-', label='Median Minutes')

# Add horizontal lines for mean and median of Total Points
plt.axhline(data['Total_Points'].mean(), color='red', linestyle='--', label='Mean Total Points')
plt.axhline(data['Total_Points'].median(), color='yellow', linestyle='-', label='Median Total Points')

sns.kdeplot(x='Minutes', y='Total_Points', data=data, cmap="Blues", fill=True, alpha=.4)

plt.legend()

# Create a scatterplot that correlates minutes played to points
fig = px.scatter(data, x='Minutes', y='Total_Points', hover_name='Player_Name',
                 title='Correlation between Minutes Played and Total Points',
                 labels={'Minutes': 'Minutes Played', 'Total_Points': 'Total Points'})

# Add vertical lines for mean and median of Minutes
fig.add_vline(x=data['Minutes'].mean(), line_width=2, line_dash="dash", line_color="royalblue", annotation_text="Mean Minutes", annotation_position="top left")
fig.add_vline(x=data['Minutes'].median(), line_width=2, line_color="springgreen", annotation_text="Median Minutes", annotation_position="bottom left")

# Add horizontal lines for mean and median of Total Points
fig.add_hline(y=data['Total_Points'].mean(), line_width=2, line_dash="dash", line_color="red", annotation_text="Mean Total Points", annotation_position="top right")
fig.add_hline(y=data['Total_Points'].median(), line_width=2, line_color="yellow", annotation_text="Median Total Points", annotation_position="bottom right")

# Calculate IQR for Total Points
Q1_tp = data['Total_Points'].quantile(0.25)
Q3_tp = data['Total_Points'].quantile(0.75)
IQR_tp = Q3_tp - Q1_tp

# Add a shaded region for the IQR of Total Points
fig.add_hrect(y0=Q1_tp, y1=Q3_tp, line_width=0, fillcolor="fuchsia", opacity=0.1, annotation_text="IQR (Total Points)", annotation_position="top right")

# Calculate IQR for Minutes
Q1_min = data['Minutes'].quantile(0.25)
Q3_min = data['Minutes'].quantile(0.75)
IQR_min = Q3_min - Q1_min

# Add a shaded region for the IQR of Minutes
fig.add_vrect(x0=Q1_min, x1=Q3_min, line_width=0, fillcolor="cyan", opacity=0.1, annotation_text="IQR (Minutes)", annotation_position="top left")

fig.update_traces(marker=dict(color='deeppink'))
fig.update_layout(yaxis_range=[0, data['Total_Points'].max() + 25], width=1200, height=800)  # Increase y-axis range for better label visibility

"""**Use the above interactive plot to see the names of players in relation to their total points scored and minutes played.**"""

# Create a scatterplot that shows points and goals scored

plt.figure(figsize=(15, 10))
sns.scatterplot(x='Goals_Scored', y='Total_Points', data=data, color='magenta')
plt.title('Scatter Plot of Goals Scored vs Total Points', color="gold", fontsize=15)
plt.xlabel('Goals Scored', color="darkorchid")
plt.ylabel('Total Points', color="blueviolet")

# Scatterplot that shows threat correlated to points in one color and  influence correlated to points in another color
plt.figure(figsize=(15, 10))

# Scatter plot for Threat vs Total Points
plt.scatter(data['Threat'], data['Total_Points'], color='gold', label='Threat vs Points')

# Scatter plot for Influence vs Total Points
plt.scatter(data['Influence'], data['Total_Points'], color='mediumorchid', label='Influence vs Points')

plt.title('Scatter Plot of Threat and Influence vs Total Points', color="gold", fontsize=15)
plt.xlabel('Threat / Influence', color="darkorchid")
plt.ylabel('Total Points', color="blueviolet")
plt.legend(loc='upper left')

# Scatterplot of Clean Sheets and  Goals Conceded correlated to total points
plt.figure(figsize=(15, 10))

# Scatter plot for Clean Sheets vs Total Points
plt.scatter(df['Clean_Sheets'], df['Total_Points'], color='gold', label='Clean Sheets vs Points')

# Scatter plot for Goals Conceded vs Total Points
plt.scatter(df['Goals_Conceded'], df['Total_Points'], color='red', label='Goals Conceded vs Points')

plt.title('Scatter Plot of Clean Sheets and Goals Conceded vs Total Points', color="crimson", fontsize=15)
plt.xlabel('Clean Sheets / Goals Conceded', color="gold")
plt.ylabel('Total Points', color="gold")
plt.legend()

# Subplot to plot all the plots at the same time
# Check for outliers

plt.figure(figsize = (20, 10))

numeric_columns = df.select_dtypes(include = np.number).columns.tolist()

for i, variable in enumerate(numeric_columns):

    plt.subplot(2, 5, i + 1)

    plt.boxplot(df[variable], whis = 1.5) #1.5 is the default

    plt.tight_layout()

    plt.title(variable)

"""**We are not going to treat the outliers for this clustering analysis because the outliers are real and valuable.**

## **Scaling**

- Clustering algorithms are distance-based algorithms, and all distance-based algorithms are affected by the scale of the variables.
- It is essential to scale all the data so that we are comparing like distances; or apples to apples so to speak.

**Scaling is typically done to standardize features so that they have a mean of 0 and a standard deviation of 1. This process is common in machine learning to ensure that features contribute equally to the model.**

- df.shape and scaled_df.shape:
  - The first line df.shape shows that the starting DataFrame has 476 rows and 10 columns.
- scaled_df.shape:
  - confirms that after scaling, the DataFrame still has 476 rows and 10 columns.
- scaled_df.head():
  - Displays the first few rows of the scaled DataFrame.
  - All the features (e.g., Goals_Scored, Assists, etc.) have been scaled
  - The values now range around 0, with positive and negative numbers indicating how far each value is from the mean of the original data.
- scaled_df.describe():
  - Gives a statistical summary of the scaled DataFrame.
- scaled_df.var():
  - This output shows the variance of each feature in the scaled DataFrame.
  - After scaling, the variance of each feature should be close to 1 (though there might be slight differences due to rounding).
"""

from sklearn.preprocessing import StandardScaler

# Scale df and create a new dataframe called scaled_df

scaler = StandardScaler()
scaled_data = scaler.fit_transform(df)
scaled_df = pd.DataFrame(scaled_data, columns=df.columns)

df.shape

scaled_df.shape

"""**Shape of the Data**:
- The dataset has 476 rows and 10 columns.
"""

scaled_df.head()

"""**Scaled Data Preview** (`scaled_df.head()`):
   - The data has been standardized, with each feature having a mean close to 0 and a standard deviation close to 1.

"""

scaled_df.describe()

"""**RAPID Summary Statistics** (`scaled_df.describe()`):
   - The mean of the scaled features is near zero, which is expected after scaling.
   - The standard deviations are all close to 1.
   - The minimum and maximum values indicate that the data has been standardized properly.

###**Understanding the numbers**
- 1.377e-17 in numbers is written as 0.00000000000000001377
    - It is same as 1.377 x 10^-17 in scientific notation.
    - You are moving the decimal point 17 places to the left.
- 4.760000e+02 is equal to \( 4.76 times 100 = 476 \).
  - The `e+02` means "times 10 to the power of 2," or multiplying by 100.
written in scientific notation.
  - The number written out is 476.
- `1.001052e+00` is 1.00105
  - It is written in scientific notation.
  - The `e+00` means "times 10 to the power of 0," which is just 1.
  - So, `1.001052e+00` is equal to \( 1.001052 times 1 = 1.001052 \).

**Data Description:**
- Mean:
  - For all columns, the mean is very close to zero, which is what we expect after standardization. Small variations from zero (like 1.492737e-17) are due to floating-point precision errors.
- Standard Deviation (std):
  - The standard deviation for each feature is very close to 1 (1.001052e+00), which confirms that the data has been properly standardized.
- Minimum (min) and Maximum (max) Values:
  - These show the range of values after standardization.
  - For example, the max value for Goals_Scored is 6.110330e+00, and the min is -5.520670e+00. These values indicate the number of standard deviations away from the mean.
- Quartiles:
 - 25%, 50% (Median), and 75% Quartiles: These provide information about the distribution of the data.
  - For example, for Goals_Scored, 25% of the data is less than -5.520670e+00, 50% (the median) is -5.520670e+00, and 75% is 2.677843e+00.
- Variance:
  - The variance for each column will be close to 1 after standardization since variance is the square of the standard deviation, and the standard deviation was standardized to 1.

**Summary:**
- Scaled Data:
  - Each feature has a mean near zero and a standard deviation near one, confirming proper standardization.
- Distribution:
 - The quartiles provide insights into how the data points are spread out across each feature.
- Extremes:
  - The min and max values show how far the extreme values are from the mean in terms of standard deviations.

The small variations from zero, such as `1.492737e-17`, are indeed due to floating-point precision errors, which occur because of the way computers handle decimal numbers. These errors are extremely small and usually negligible in most practical situations.

Here’s why you typically **don’t need to correct these errors**:

1. **Negligible Impact**: The variations are so small that they have no meaningful impact on your analysis or the results you derive from the data.

2. **Precision Limits**: These tiny errors are inherent to how numbers are represented in computers. Trying to "correct" them often leads to unnecessary complexity without any real benefit.

3. **Data Consistency**: If you force these tiny variations to zero, you might introduce unintended artifacts or inconsistencies in your data, especially if further calculations depend on this standardized data.

**When Should You Consider Correction?**
- If your analysis is extremely sensitive to precision (like in high-precision scientific computing), you might consider additional steps to handle these small variations, such as rounding.
##Common Practice
  - In most cases, analysts and data scientists accept these small errors as part of working with numerical data and proceed without correcting them.
  
  **If you're still concerned or working in a context where even the smallest errors could propagate in a significant way, you might consider rounding your standardized data to a reasonable number of decimal places (e.g., 15 or 16).**

### **Variance (scaled_df.var()):**
   - **What is variance?** Variance is a measure of how much the values in a dataset differ from the mean (average) of the dataset. Specifically, it gives you an idea of how spread out the data points are.
   - **What does it mean in the context of scaled data?** After scaling your data (typically through standardization), each feature (or column) should have a mean of 0 and a variance close to 1. This ensures that all features contribute equally to any analysis, like clustering, without being dominated by features that have larger original scales.

**scaled_df.var():**
  - This output shows the variance of each feature in the scaled DataFrame.
  - After scaling, the variance of each feature should be close to 1 (though there might be slight differences due to rounding).

**Variance Check** (`scaled_df.var()`):
   - The variances of the features are all close to 1, which is exactly what we want to see after scaling.
   - Everything seems to be in order with the scaled data.
"""

scaled_df.var()

"""**Numbers in the Variance of this DataSet:**
   - Each feature in the dataset (like "Goals_Scored," "Assists," etc.) has a variance value of approximately `1.002105`.

**Representation of the numbers:**
- `1.002105` means the variance of that feature is slightly above 1, which is expected and perfectly normal after scaling. The small difference from exactly 1 is due to rounding and the precision of the calculation.
- All features having similar variance values around 1 indicates that the scaling process was successful.

**You don't have to worry about a single feature disproportionately influencing the clustering process.**

### **Why is this Important?**
   - **Standardized Data:** Clustering algorithms like K-Means are sensitive to the scale of the data. If the features are not scaled (e.g., one feature is measured in millions while another is measured in single digits), the algorithm might consider the feature with the larger scale as more important. By ensuring all features have a similar variance (around 1), you make sure each feature is treated equally during clustering.

The data has been successfully standardized, which is crucial for algorithms that are sensitive to the scale of the data, like K-Means clustering or Principal Component Analysis (PCA).

Uniform Contribution:
- After scaling, all features now contribute equally to the analysis or model, as they are all on the same scale.

Ready for Modeling:
- With the data scaled, you are ready to proceed with clustering, PCA, or any other modeling techniques that require standardized inputs.

Practical Implications:
- The scaled values themselves aren’t directly interpretable in the same way as the original data, but they are essential for ensuring that the model treats all features equally.
- After clustering, you will assign a cluster number to a new DataFrame that will now have the prescaled data, as well as an assigned cluster number added as a feature per each row.

###**Collinearity**

Collinearity is a statistical phenomenon that occurs when two or more predictor variables in a multiple regression model are highly correlated. This means that one variable can be accurately predicted from the others using a linear relationship.

Collinearity consequences:
- Can cause problems in estimating regression coefficients, which can lead to unreliable and unstable results.
- Can also make it difficult to estimate the individual regression coefficients of the variables reliably. When collinearity is present, the predictor variables cannot independently predict the value of the dependent variable. This is because they explain some of the same variance in the dependent variable, which reduces their statistical significance.
- Increases the time required to run the analysis because of the presence of unnecessary data.

**Here are some ways to detect collinearity:**
- Correlation matrix:
  - Look for independent variables with a Pearson's correlation above 0.9. This indicates that the variables are highly correlated and therefore collinear.
- Eigenvalues:
  - If one of the eigenvalues in the data is much smaller than the others, this could indicate collinearity. To calculate the eigenvalues, you can use the eigvals() method in the Numpy library.
- PCA
 - Principal component analysis can be useful when dealing with collinearity issues.

## **Principle Component Analysis (PCA): Step by Step**

- **Determining Number of Principal Components**:
  ```python
  n = subset.shape[1]
  ```
  - This assigns `n` as the number of original features, which is typical when you want to reduce the dimensionality while preserving as much variance as possible. It’s a good default, but you might want to allow for flexibility here if you want to experiment with fewer components.

- **Applying PCA**:
  ```python
  pca = PCA(n_components = n, random_state = 1)
  data_pca = pd.DataFrame(pca.fit_transform(subset_scaled_df))
  ```
  - This correctly applies PCA and stores the transformed data in `data_pca`.
  - **Improvement**: You could name the columns in `data_pca` for easier reference, e.g., `columns=[f'PC{i+1}' for i in range(n)]`. This way, the principal components are labeled clearly in the DataFrame.
    - Consider naming the principal components in `data_pca` for easier readability.

- **Calculating Explained Variance**:
  ```python
  exp_var = (pca.explained_variance_ratio_)
  ```
  - This stores the explained variance ratio, which is crucial for understanding how much of the original variance each principal component captures.

**Additional Suggestions**:

- **Explained Variance Plot**:
  - After calculating `exp_var`, you might consider plotting it to visualize how much variance each principal component explains. This can help in deciding whether you need all components or can reduce further.
  - Allow for flexibility in `n_components` if you wish to explore reducing the number of dimensions further.
  
  ```python
  import matplotlib.pyplot as plt

  plt.figure(figsize=(10, 5))
  plt.bar(range(1, len(exp_var)+1), exp_var, alpha=0.5, align='center')
  plt.ylabel('Variance Explained')
  plt.xlabel('Principal Components')
  plt.title('Variance Explained by Each Principal Component')
  plt.show()
  ```

- **Dimensionality Reduction**:
  - If you find that the first few components explain most of the variance, you might want to set `n_components` to a smaller number to reduce the dimensionality further while retaining most of the information.
"""

# Create a heatmap of the standardized data using Seaborn's heatmap function
plt.figure(figsize=(10, 10))
sns.heatmap(scaled_df.corr(), annot=True, cmap='seismic')
plt.title('Correlation Heatmap of Standardized Data', color="red")

"""**Observations:**
The heatmap shows that certain features are highly correlated with each other.
- Total_Points and Minutes:
  - There is a very high correlation (0.91), indicating that these two variables likely provide similar information. Including both might introduce redundancy in the PCA model.
- Total_Points and Influence:
  - Another high correlation (0.96), suggesting a strong relationship between these two variables. Again, this redundancy could be problematic.
- Minutes and Goals_Conceded:
  - These are also highly correlated (0.94). Depending on our analysis goals, we might consider dropping one of these features.
- Goals_Scored and Threat:
  - The correlation here is 0.9, which is high enough to consider whether both are needed.
- Influence and Goals_Conceded:
  - This pair also shows a high correlation (0.86), but does fall slightly below our (0.9) threshhold.

**In general, for PCA, you’d want to reduce the number of features that are highly correlated to avoid issues with multicollinearity, which can distort the component loadings and make the results harder to interpret.**
"""

# List the features that correlate above (0.9) with each other

# Calculate the correlation matrix
corr_matrix = scaled_df.corr()

# Find pairs with correlation above 0.9
high_corr_pairs = []
for i in range(len(corr_matrix.columns)):
  for j in range(i + 1, len(corr_matrix.columns)):
    if abs(corr_matrix.iloc[i, j]) > 0.9:
      pair = (corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j])
      high_corr_pairs.append(pair)

# Print the pairs in a readable format
print("Highly correlated pairs (above 0.9):")
for pair in high_corr_pairs:
  print(f"{pair[0]} and {pair[1]}: {pair[2]:.2f}")

"""Observations:
1. Goals_Scored and Threat: Consider keeping one, depending on whether you want to emphasize overall scoring ability or a more nuanced measure of offensive potential.
2.	Total_Points and Minutes: Our plots revealed that more minutes = more errors; and that some players are exceptional regardless of minutes. This might create a distortion, so minutes would be a good feature to drop.
3.	Total_Points and Influence: Since this pair has the highest correlation (0.96), dropping one could significantly reduce redundancy.
4.	Minutes and Goals_Conceded: Consider which aspect of gameplay (time on field vs. defensive lapses) is more critical for your analysis.
5.	Minutes and Influence: Another pair where dropping one might help, given their strong relationship.
6.	Minutes and Clean_Sheets: Again, deciding between the importance of playtime versus defensive success.

"""

# Find the features that correlate above 90 and list them vertically, with the total number of outliers per feature

# Calculate the correlation matrix
corr_matrix = scaled_df.corr()

# Find pairs with correlation above 0.9
high_corr_pairs = []
for i in range(len(corr_matrix.columns)):
  for j in range(i + 1, len(corr_matrix.columns)):
    if abs(corr_matrix.iloc[i, j]) > 0.9:
      pair = (corr_matrix.columns[i], corr_matrix.columns[j])
      high_corr_pairs.append(pair)

# Extract individual features from the pairs
high_corr_features = set()
for pair in high_corr_pairs:
  high_corr_features.add(pair[0])
  high_corr_features.add(pair[1])

# Print features vertically and count outliers
print("Highly correlated features (above 0.9):")
for feature in high_corr_features:
  print(feature)

  # Calculate outliers using IQR method
  Q1 = scaled_df[feature].quantile(0.25)
  Q3 = scaled_df[feature].quantile(0.75)
  IQR = Q3 - Q1
  lower_bound = Q1 - 1.5 * IQR
  upper_bound = Q3 + 1.5 * IQR
  outliers = scaled_df[(scaled_df[feature] < lower_bound) | (scaled_df[feature] > upper_bound)]
  print(f"  Number of outliers: {len(outliers)}")

"""Given the outlier counts in our highly correlated features, here are some thoughts on how to proceed:
- Goals_Scored (57 outliers):
 - This large number of outliers suggests that player performance in terms of goals is highly variable.
 - Goals_Scored is crucial for our analysis. We might want to treat the outliers. It might be better to consult with a domain expert.
- Threat (40 outliers):
 - Highly correlated and has a lot of outliers.
 - We can drop Threat.
- Total_Points (4 outliers) and Influence (2 outliers):
 - With fewer outliers, these variables might still be reliable indicators.
- Clean_Sheets, Goals_Conceded, and Minutes (0 outliers):
  - These features are free of outliers and likely provide stable, consistent information. They can be retained as-is.
"""

# Print a list of our {len(outliers)} that == 0 and their correlation
# {len(outliers)} = features with a correlation > (0.90)
# Calculate the correlation matrix
corr_matrix = scaled_df.corr()

# Find features with 0 outliers and their correlations
no_outlier_features = []
for feature in scaled_df.columns:
  Q1 = scaled_df[feature].quantile(0.25)
  Q3 = scaled_df[feature].quantile(0.75)
  IQR = Q3 - Q1
  lower_bound = Q1 - 1.5 * IQR
  upper_bound = Q3 + 1.5 * IQR
  outliers = scaled_df[(scaled_df[feature] < lower_bound) | (scaled_df[feature] > upper_bound)]
  if len(outliers) == 0:
    no_outlier_features.append(feature)

print("Features with 0 outliers and their correlations:")
for i in range(len(no_outlier_features)):
  for j in range(i + 1, len(no_outlier_features)):
    feature1 = no_outlier_features[i]
    feature2 = no_outlier_features[j]
    correlation = corr_matrix.loc[feature1, feature2]
    print(f"{feature1} and {feature2}: {correlation:.2f}")

"""**We have confirmed that we should drop minutes both from our visual observations and via a formulaic mathematical path.**"""

# Create a copy of the scaled data
scaled_df_no_minutes = scaled_df.copy()

# Drop the 'Minutes' column
scaled_df_no_minutes = scaled_df_no_minutes.drop('Minutes', axis=1)

# Original Dataset
data.head(0)

# Numerical Dataset
df.head(0)

# Scaled df retains the same number of features as df
scaled_df.head(0)

# Sacaled df no minutes has dropped the minutes feature
scaled_df_no_minutes.head(0)

# Rename scaled_df_no_minute to df_pca copy
df_pca = scaled_df_no_minutes.copy()

df_pca.head(0)

# Import the PCA class from sklearn.decomposition
from sklearn.decomposition import PCA

# Fit the PCA model (assuming 'df_pca' is your dataset)
pca = PCA(n_components=len(df_pca.columns))
pca.fit(df_pca)

# Extract the explained variance ratio
exp_var = pca.explained_variance_ratio_

# Plot explained variance using a barplot
plt.figure(figsize=(10, 5))

# Use a colormap for the bars
colors = cm.flag(np.linspace(0, 1, len(exp_var)))
plt.bar(range(1, len(exp_var) + 1), exp_var, alpha=0.8, align='center', color=colors)

plt.ylabel('Variance Explained')
plt.xlabel('Principal Components')
plt.title('Variance Explained by Each Principal Component', color = "blue")

# Add gridlines for better readability
plt.grid(axis='y', linestyle='--', alpha=0.7)

"""Observations:
- First Principal Component (PC1):
  - This component explains over 70% of the variance in the data, which is substantial. It suggests that the first principal component captures most of the important information from the dataset.
- Second Principal Component (PC2):
  - The second component explains a much smaller portion of the variance, around 10-15%. This indicates that while PC2 still holds some relevant information, it’s much less significant compared to PC1.
- Subsequent Components:
  - The remaining components each explain progressively smaller amounts of variance, suggesting that they are capturing relatively minor variations or noise in the data.
"""

# Create silhouette score plot
kmeans_kwargs = {"init": "random", "n_init": 10, "max_iter": 300, "random_state": 42,}
silhouette_scores = []
for k in range(2, 11):
  kmeans = KMeans(n_clusters=k, **kmeans_kwargs)
  kmeans.fit(df_pca)
  score = silhouette_score(df_pca, kmeans.labels_)
  silhouette_scores.append(score)

# Plot the silhouette scores
plt.figure(figsize=(8, 3))
plt.plot(range(2, 11), silhouette_scores, marker='o', color="red")
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Silhouette Score")
plt.title("Silhouette Score vs Number of Clusters", color="blue")

"""**Interpretation of the Silhouette Score Plot:**
- Highest Score at 2 Clusters:
  - The plot shows the highest Silhouette Score at 2 clusters, indicating that the data might naturally form two distinct groups.
- Decreasing Scores:
  - As the number of clusters increases, the Silhouette Score generally decreases, which suggests that the data may not be well-separated into more than two clusters.
- Elbow or Plateau:
  - After 3 clusters, the scores level off somewhat, indicating that adding more clusters beyond 3 might not provide significantly better separation.

**Optimal Number of Clusters: 2**
  - Based on the plot, 2 clusters seem to be the optimal number, as they provide the highest Silhouette Score, meaning the data points are best clustered within this number of clusters.
  
**You might also consider 3 clusters if you have domain knowledge that suggests this number could be meaningful. Any more than 3 clusters seem to result in less clear grouping.**

## **K-Means Clustering**

- K-Means clustering is one of the most popular algorithms for partitioning a dataset into K clusters. The algorithm works by iteratively assigning each data point to one of the K clusters based on the proximity of the data points to the cluster centroids. K-Means is computationally efficient and can perform well even with datasets that have a large number of variables.

- The steps involved in K-Means clustering are as follows:
    - Choose the number of clusters K that you want to partition the data into.
    - Initialize the K centroids randomly.
    - Assign each data point to the nearest centroid.
    - Recalculate the centroids of each cluster as the mean of all the data points assigned to it.
    - Repeat steps 3 and 4 until the centroids no longer change or a maximum number of iterations is reached.
"""

# Create a copy of the dataset and rename for K-Means Clustering
k_means2_df = df_pca.copy()

# Run K-Means Clustering, clusters = 2

# Apply K-Means with 2 clusters
kmeans = KMeans(n_clusters=2, random_state=1)
k_means2_df['Cluster'] = kmeans.fit_predict(k_means2_df)

# Calculate silhouette score
silhouette_score(k_means2_df, kmeans.labels_)

# Create a copy of the dataset and rename for K-Means Clustering
k_means3_df = df_pca.copy()

# Run K-Means Clustering, clusters = 3

# Apply K-Means with 3 clusters
kmeans = KMeans(n_clusters=3, random_state=1)
k_means3_df['Cluster'] = kmeans.fit_predict(k_means3_df)

# Calculate silhouette score
silhouette_score(k_means3_df, kmeans.labels_)

# Create a copy of the dataset and rename for K-Means Clustering
k_means4_df = df_pca.copy()

# Run K-Means Clustering, clusters = 4

# Apply K-Means with 4 clusters
kmeans = KMeans(n_clusters=4, random_state=1)
k_means4_df['Cluster'] = kmeans.fit_predict(k_means4_df)

# Calculate silhouette score
silhouette_score(k_means4_df, kmeans.labels_)

df_pca.head(0)

k_means3_df.head(4)

# Group the data by cluster
grouped = k_means3_df.groupby('Cluster')

# Set up the figure and color palette
plt.figure(figsize=(10, 6))
colors = plt.cm.viridis(np.linspace(0, 1, len(grouped)))

# Plot each cluster
for (key, group), color in zip(grouped, colors):
    plt.scatter(group.iloc[:, 0], group.iloc[:, 1], label=f'Cluster {key}', color=color)

# Add titles and labels
plt.title('K-Means Clustering Results')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend()

from sklearn.decomposition import PCA

# Initialize PCA
pca = PCA(n_components=len(df.columns))  # or specify the number of components if you want fewer

# Fit PCA on the scaled data
data_pca = pca.fit_transform(scaled_df)

# Convert the PCA result to a DataFrame
data_pca = pd.DataFrame(data_pca, columns=[f'PC{i+1}' for i in range(data_pca.shape[1])])

print(data.head(1))
print(df_pca.head(1))   # Shows the first few rows of df_pca
print(data_pca.head(1)) # Shows the first few rows of data_pca
print(k_means3_df.head(1)) # Shows the first few rows of k_means3_df

"""### **Cluster Profiles**"""

# List the names of each player by cluster

# 'data' is the original DataFrame with player names and
# 'k_means3_df' has the cluster assignments

# Add cluster assignments to the original data
data_with_clusters = data.copy()
data_with_clusters['Cluster'] = k_means3_df['Cluster']

# Group by cluster and display player names
for cluster_num, group in data_with_clusters.groupby('Cluster'):
  print(f"Players in Cluster {cluster_num}:")
  for player in group['Player_Name']:
    print(f"  - {player}")
  print("\n")

# Create a scatter plot where x,y axis = PCA1, PCA2, the data points are scaled and clustered using Kmeans, and hover = player name
# 'data_pca' has the principal components, 'k_means3_df' has the 'Cluster' column
# 'data' has the 'Player_Name' column
# Add player names to the data_pca DataFrame
# data_pca['Player_Name'] = data['Player_Name']
# Create the interactive scatterplot
# fig = px.scatter(data_pca, x='PC1', y='PC2', color=k_means3_df['Cluster'], hover_data=['Player_Name'])

# Add player names to the data_pca DataFrame
data_pca['Player_Name'] = data['Player_Name']

# Create the interactive scatterplot
fig = px.scatter(data_pca, x='PC1', y='PC2', color=k_means3_df['Cluster'],
                 hover_name='Player_Name',  # Add hover_name for player names
                 title='K-Means Clustering Results',
                 labels={'PC1': 'Principal Component 1', 'PC2': 'Principal Component 2'})
fig.show()

# Create cluster profile matrix

# Calculate the mean of each feature within each cluster
cluster_profiles = k_means3_df.groupby('Cluster').mean()

# Print the cluster profiles
print(cluster_profiles)

"""**Cluster Profiles:**

- Cluster 0:
  - Goals_Scored: 1.9641 (Highest among the clusters)
  - Assists: 1.841388 (Highest among the clusters)
	- Total_Points: 1.618901 (Highest among the clusters)
	- Creativity: 1.706084 (Highest among the clusters)
	- Influence: 1.36462 (Highest among the clusters)
	- Threat: 1.945958 (Highest among the clusters)
	- Bonus: 1.8337 (Highest among the clusters)
	- Clean_Sheets: 1.051002 (Highest among the clusters)
- Cluster 1:
  - Goals_Scored: -0.455654 (Negative, indicating below-average performance)
  - Assists: -0.502999 (Negative, indicating below-average performance)
  - Total_Points: -0.794588 (Negative, indicating below-average performance)
  - Creativity: -0.577514 (Negative, indicating below-average performance)
  - Influence: -0.793776 (Negative, indicating below-average performance)
  - Threat: -0.53184 (Negative, indicating below-average performance)
  - Bonus: -0.623093 (Negative, indicating below-average performance)
  - Clean_Sheets: -0.779831 (Lowest among the clusters)
- Cluster 2:
	- Goals_Scored: -0.07744 (Close to zero, indicating average performance)
	- Assists: 0.042042 (Close to zero, indicating average performance)
	- Total_Points: 0.055659 (Close to zero, indicating average performance)
	- Creativity: 0.207524 (Slightly above average)
	- Influence: 0.667091 (Above average, but not as high as Cluster 0)
	- Threat: 0.044488 (Close to zero, indicating average performance)
	- Bonus: 0.226649 (Slightly above average)
	- Clean_Sheets: 0.785539 (Above average among the clusters)

**Interpretation:**
- Cluster 0:
  - Appears to represent the top-performing players across the board.
  - These players have the highest scores in offensive metrics such as Goals Scored, Assists, Total Points, Creativity, Influence, and Threat.
  - They are likely key contributors to their teams, excelling in both attacking and playmaking roles.
  - These players are likely the stars or key players who consistently perform at a high level.
  - They have a high number of goal conceded which suggests that these players are not defense players.

**Interpretation:**
- Cluster 1
  - Seems to represent players who are underperforming or less impactful.
  - The negative values across almost all metrics suggest that these players contribute less to their teams in terms of goals, assists, and overall influence.
  - They may be players who are struggling in their roles or perhaps play in more defensive positions with less opportunity to contribute to offensive stats.
  - The very low Clean Sheets score might also suggest that this cluster includes players from teams that concede more goals.

**Interpretation:**
	- Cluster 2 appears to represent players with average or balanced performance
  - These players are not excelling as much as those in Cluster 0, but they are also not underperforming like those in Cluster 1.
  - The high Clean Sheets value suggests that this cluster might include players with stronger defensive contributions.
  - This cluster could represent solid, reliable players who contribute to their teams in various ways without necessarily standing out as top performers.

**Summary:**
- Cluster 0: Top performers, likely star players with strong offensive contributions.
- Cluster 1: Underperformers, potentially struggling players or those in more defensive roles with lower contributions to offensive metrics.
- Cluster 2: Average performers, balanced players with decent contributions, particularly in defensive metrics like Clean Sheets.

**Let's plot the boxplot**
"""

# Create a subplot that holds the boxplots for each of the features in data=k_means3_df, pca dataset = df_pca

# Create subplots for each feature
fig, axes = plt.subplots(3, 3, figsize=(15, 10))

# Flatten the axes array for easier iteration
axes = axes.flatten()

# Color palette for clusters
colors = ['purple', 'yellow', 'lavender']

# Iterate over features and create boxplots
for i, feature in enumerate(df_pca.columns):
  for cluster in k_means3_df['Cluster'].unique():
    cluster_data = k_means3_df[k_means3_df['Cluster'] == cluster][feature]
    axes[i].boxplot(cluster_data, positions=[cluster + 1], patch_artist=True,
                    boxprops=dict(facecolor=colors[cluster]), widths=0.5)

  axes[i].set_title(feature)
  axes[i].set_xticks(k_means3_df['Cluster'].unique() + 1)
  axes[i].set_xticklabels([f'Cluster {cluster}' for cluster in k_means3_df['Cluster'].unique()])

plt.tight_layout()

"""### **Characteristics of each cluster:**

The box plots provide a great visual summary of how each feature varies across the three clusters. Let’s go through these plots feature by feature to deepen the interpretation:

- Goals Scored:
  - Cluster 0: The highest median and range, with several outliers. This cluster clearly consists of players who score the most goals.
  - Cluster 1: Almost no goals scored, indicating players who rarely or never score.
  - Cluster 2: A mix of players with some goal-scoring ability, but far below those in Cluster 0.

Interpretation:
- Cluster 0 is made up of top goal scorers
- Cluster 1 has very low to no goal-scoring contribution
- Cluster 2 is more average

- Assists:
  - Cluster 0: High median and range, showing players with strong assisting ability.
  - Cluster 1: Very low median, with only a few players providing assists.
  - Cluster 2: Lower than Cluster 0 but better than Cluster 1, indicating moderate assisting abilities.

Interpretation:
  - Similar to goals, Cluster 0 dominates in assists, likely representing playmakers.
  - Cluster 1 has little to no assist contribution
  - Cluster 2 sits in between.

- Total Points:
  - Cluster 0: High total points, consistent with their performance in goals and assists.
  - Cluster 1: Very low total points, aligning with their low performance across other metrics.
  - Cluster 2: Moderate total points, indicating balanced but not top-tier performance.

Interpretation:
  - Cluster 0 likely represents star players across multiple metrics.
  - Cluster 1 has players contributing the least overall.
  - Cluster 2 is more balanced.

- Goals Conceded:
  - Cluster 0: Higher goals conceded, suggesting these players might be more offensively focused, possibly at the expense of defense.
  - Cluster 1: Very low goals conceded, possibly indicating more defensive or less engaged roles.
  - Cluster 2: Moderate, but higher than Cluster 1, indicating some defensive responsibilities.

Interpretation:
  - Cluster 0 might include more offensive players with less focus on defense.
  - Cluster 1 has more defensively oriented players.
  - Cluster 2 has the stronger defensive players.

Creativity:
- Cluster 0: High creativity, consistent with their high assists and goals.
- Cluster 1: Low creativity, aligning with their low overall contribution.
- Cluster 2: Moderate creativity, indicating some creative contribution but not at Cluster 0’s level.

Interpretation:
- Cluster 0 players are likely playmakers and key contributors to their teams’ offensive creativity.
- Cluster 1 players are less involved creatively.

Influence:
- Cluster 0: High influence, indicating these players are central to their team’s success.
- Cluster 1: Very low influence, suggesting these players play more minor or supportive roles.
- Cluster 2: Moderate influence, showing a balanced contribution.

Interpretation:
- Cluster 0 likely represents players who are key figures in their teams, while Cluster 1 has players with minimal influence on outcomes.

Threat:
- Cluster 0: High threat level, consistent with their offensive prowess.
- Cluster 1: Low threat, suggesting minimal offensive pressure.
- Cluster 2: Moderate threat, indicating some offensive potential.

Interpretation:
- Cluster 0 players are dangerous in offensive positions, while Cluster 1 players contribute little threat.

Bonus:
- Cluster 0: High bonus points, indicating they frequently excel in games.
- Cluster 1: Very low bonus points, reflecting their lower performance.
- Cluster 2: Moderate bonus points, suggesting occasional standout performances.

Interpretation:
- Cluster 0 players are consistently high performers, while Cluster 1 struggles to stand out.

Clean Sheets:
- Cluster 0: Highest clean sheets, indicating a strong defensive team.
- Cluster 1: Very low clean sheets, perhaps reflecting weaker defensive players or those who play less defensively.
- Cluster 2: Very good clean sheets, suggesting their primary defensive contributions.

Interpretation:
- Cluster 2 likely includes stronger defensive players, while Cluster 1 struggles defensively.

Overall Summary:
- Cluster 0: Contains top-performing players, excelling in offensive and creative metrics, with strong influence and consistent standout performances.
- Cluster 1: Represents underperforming or more defensively oriented players with low contributions across all metrics.
- Cluster 2: Balanced players, contributing moderately across the board, with a slight emphasis on defensive performance.

## **K-Medoids Clustering**

- K-Medoids clustering is a variant of K-Means clustering that uses medoids instead of centroids to define the clusters. Medoids are data points within a cluster that have the minimum average dissimilarity to all the other points in the cluster.

- The steps involved in K-Medoids clustering are as follows:
    - Choose the number of clusters K that you want to partition the data into.
    - Initialize K medoids randomly.
    - Assign each data point to the nearest medoid.
    - For each medoid, compute the average dissimilarity to all the other points in the cluster.
    - For each medoid and non-medoid pair, swap the medoid and non-medoid and compute the new total dissimilarity of the cluster.
    - If the total dissimilarity decreases after the swap, keep the new medoid, otherwise keep the old medoid.
    - Repeat steps 3 to 6 until the medoids no longer change or a maximum number of iterations is reached.

K-Medoids clustering is a robust algorithm that can handle non-linear clusters and is less sensitive to outliers compared to K-Means clustering. However, it can be computationally expensive for large datasets, as it requires computing the pairwise dissimilarities between all the data points.
"""

# Import k metoids

from sklearn_extra.cluster import KMedoids

# Show pca dataframe

df_pca.head(3)

"""Choosing the number of clusters (K) is a critical step in clustering, and the method you use can significantly impact the results. Since you’ve already used 3 clusters for K-means, you might be inclined to compare the results directly by using the same number of clusters for K-medoids. However, it’s also worth considering whether another number of clusters might be more appropriate for the specific structure of your data.

Methods to Determine the Optimal Number of Clusters:
Elbow Method:
- How it works: Plot the total within-cluster sum of squares (WCSS) against the number of clusters. The point where the decrease in WCSS starts to slow down (forming an “elbow”) indicates the optimal number of clusters.
- Application: You can apply the elbow method to both K-means and K-medoids to compare and choose the optimal number of clusters.
  - **NOTE: You have already completed this step.**

Silhouette Score:
- How it works: The silhouette score measures how similar a point is to its own cluster compared to other clusters. It ranges from -1 to 1, where a higher value indicates better-defined clusters.
- Application: Calculate the silhouette score for different numbers of clusters and choose the K with the highest score.

Consistency with K-Means:
- How it works: If you want to directly compare the results of K-medoids with K-means, using the same number of clusters (3) can make it easier to compare and contrast the outcomes.

**Start with K = 3 to compare with your K-means results, and then explore 4 clusters if the data suggests it could be beneficial.**

##Evaluate and Experiment: If the results seem suboptimal (e.g., poorly defined clusters, high within-cluster variance), try changing the number of clusters.
"""

sil_scores = []
K = range(2, 10)
for k in K:
    kmedoids = KMedoids(n_clusters=k, random_state=32).fit(df_pca)
    labels = kmedoids.labels_
    sil_scores.append(silhouette_score(df_pca, labels))

plt.figure(figsize=(8, 3))
plt.plot(K, sil_scores, 'bx-', color="red")
plt.xlabel('Number of clusters')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Score for Optimal K', color="blue")

"""You're going to run K-Metiods with 2, 3 and 6 clusters.
Before you do this, create a copy of your dataframe and label it accordingly for each of the clusters.
"""

# Create a copy of the dataframe for K-Metiods, n_clusters = 2, 3 and 6

k_med2_df = df_pca.copy()
k_med3_df = df_pca.copy()
k_med6_df = df_pca.copy()

# Initialize the K-Medoids model with 2 clusters
kmedoids2 = KMedoids(n_clusters=2, random_state=1)

# Fit the model on the DataFrame
kmedoids2.fit(k_med2_df)

# Display the cluster centers (medoids)
# Remove 'Cluster_2' from the columns used to create the DataFrame
medoid_centers2 = pd.DataFrame(kmedoids2.cluster_centers_, columns=k_med2_df.columns)
print(medoid_centers2)

# Add the cluster labels to the DataFrame
k_med2_df['Cluster_2'] = kmedoids2.labels_

# Initialize the K-Medoids model with 3 clusters
kmedoids3 = KMedoids(n_clusters=3, random_state=1)

# Fit the model on the DataFrame
kmedoids3.fit(k_med3_df)

# Add the cluster labels to the DataFrame
k_med3_df['Cluster_3'] = kmedoids3.labels_

# Display the cluster centers (medoids)
medoid_centers3 = pd.DataFrame(kmedoids3.cluster_centers_, columns=k_med3_df.columns)
print(medoid_centers3)

# Initialize the K-Medoids model with 6 clusters
kmedoids6 = KMedoids(n_clusters=6, random_state=1)

# Fit the model on the DataFrame
kmedoids6.fit(k_med6_df)

# Add the cluster labels to the DataFrame
k_med6_df['Cluster_6'] = kmedoids6.labels_

# Display the cluster centers (medoids)
medoid_centers6 = pd.DataFrame(kmedoids6.cluster_centers_, columns=k_med6_df.columns)
print(medoid_centers6)

"""Comparing 2, 3, and 6 Clusters:

- 2 Clusters:
  - This solution provides a broad overview of the data with clear separation between two major groups.
- 3 Clusters:
  - Adds more nuance, splitting one of the broad clusters from the 2-cluster solution into two more defined subgroups. Groups 1 and 2 have the clearest separation.
- 6 Clusters:
  - Offers the most detailed segmentation but may introduce complexity. Some clusters may be very similar, suggesting the possibility of over-segmentation. The interperability is difficult for the human eye.

Next Steps:
- Visualize the Clusters:
  - Use these medoids to plot the clusters and visualize how the data is grouped in each of the configurations. This can help you see if the clusters are meaningful or if some of them might be too similar.
- Evaluate the Medoids:
  - Compare the medoids across the different cluster counts to determine if the additional clusters in the 3- and 6-cluster solutions provide valuable distinctions or if they might be overfitting the data.
- Decision Making:
  - Consider whether the additional granularity in the 3- and 6-cluster solutions provides actionable insights or if the broader 2-cluster solution is sufficient.

"""

# Create the scatterplot for 2 K-Metiod clusters
# Add the cluster labels to the pca_df DataFrame from kmedoids2
data_pca['Cluster_2'] = kmedoids2.labels_
# Create the scatter plot
plt.figure(figsize=(10, 6))
sns.scatterplot(x='PC1', y='PC2', hue='Cluster_2', data=data_pca, palette='inferno', s=100)

# Add titles and labels
plt.title('K-Medoids Clustering with 2 Clusters', color="cyan", fontsize=15)
plt.xlabel('Principal Component 1 (PCA1)')
plt.ylabel('Principal Component 2 (PCA2)')

# Display the plot with legend
plt.legend()

# Create the scatterplot for 3 K-Metiod clusters
# Add the cluster labels to the pca_df DataFrame from kmedoids2
data_pca['Cluster_3'] = kmedoids3.labels_
# Create the scatter plot
plt.figure(figsize=(10, 6))
sns.scatterplot(x='PC1', y='PC2', hue='Cluster_3', data=data_pca, palette='plasma', s=100)

# Add titles and labels
plt.title('K-Medoids Clustering with 3 Clusters', color="cyan", fontsize=15)
plt.xlabel('Principal Component 1 (PCA1)')
plt.ylabel('Principal Component 2 (PCA2)')

# Display the plot with legend
plt.legend()

# Create the scatterplot for 6 K-Metiod clusters
# Add the cluster labels to the pca_df DataFrame from kmedoids2
data_pca['Cluster_6'] = kmedoids6.labels_
# Create the scatter plot
plt.figure(figsize=(10, 6))
sns.scatterplot(x='PC1', y='PC2', hue='Cluster_6', data=data_pca, palette='viridis', s=100)

# Add titles and labels
plt.title('K-Medoids Clustering with 6 Clusters', color="cyan", fontsize=15)
plt.xlabel('Principal Component 1 (PCA1)')
plt.ylabel('Principal Component 2 (PCA2)')

# Display the plot with legend
plt.legend()

"""**Observations:**
As we predicted and have already stated, the clearest separation is seen using 2 clusters. We might want to keep 3 clusters for a more nuanced look at our data. While 6 clusters may offer more detail, that level of detail may also offer too much complexity from which to draw simple and actionable conclusions.
"""

# Check unique cluster labels
print(k_med2_df['Cluster_2'].unique())
print(k_med3_df['Cluster_3'].unique())
print(k_med6_df['Cluster_6'].unique())

"""**Cluster Profile Analysis:**
- Goal:
  - Understand the characteristics of each cluster.
- Action:
  - Create summary statistics (e.g., mean, median) or box plots for each feature within each cluster. This will help you identify what distinguishes one cluster from another.
- Why:
  - This step is essential to ensure that the clusters make sense and are meaningful. For example, you might find that certain clusters are dominated by specific features, indicating a natural grouping within the data.
"""

# Summary statistics for 2 clusters
summary_stats_2 = k_med2_df.groupby('Cluster_2').describe().transpose()
print("Summary Statistics for 2 Clusters:")
print(summary_stats_2)

# Summary statistics for 3 clusters
summary_stats_3 = k_med3_df.groupby('Cluster_3').describe().transpose()
print("\nSummary Statistics for 3 Clusters:")
print(summary_stats_3)

# Summary statistics for 6 clusters
summary_stats_6 = k_med6_df.groupby('Cluster_6').describe().transpose()
print("\nSummary Statistics for 6 Clusters:")
print(summary_stats_6)

# Create a subplot using boxplots for K-Metiods, clusters = 2

# Create subplots for each feature
fig, axes = plt.subplots(3, 3, figsize=(15, 10))

# Flatten the axes array for easier iteration
axes = axes.flatten()

# Color palette for clusters (using same colors as before)
colors = ['purple', 'yellow']

# Iterate over features and create boxplots
for i, feature in enumerate(df_pca.columns):
  for cluster in k_med2_df['Cluster_2'].unique():
    cluster_data = k_med2_df[k_med2_df['Cluster_2'] == cluster][feature]
    axes[i].boxplot(cluster_data, positions=[cluster + 1], patch_artist=True,
                    boxprops=dict(facecolor=colors[cluster]), widths=0.5)

  axes[i].set_title(feature)
  axes[i].set_xticks(k_med2_df['Cluster_2'].unique() + 1)
  axes[i].set_xticklabels([f'Cluster {cluster}' for cluster in k_med2_df['Cluster_2'].unique()])

plt.tight_layout()

# Create a subplot using boxplots for K-Metiods, clusters = 3

# Create subplots for each feature
fig, axes = plt.subplots(3, 3, figsize=(15, 10))

# Flatten the axes array for easier iteration
axes = axes.flatten()

# Color palette for clusters
colors = ['purple', 'yellow', 'lavender']

# Iterate over features and create boxplots
for i, feature in enumerate(df_pca.columns):
  for cluster in k_med3_df['Cluster_3'].unique():
    cluster_data = k_med3_df[k_med3_df['Cluster_3'] == cluster][feature]
    axes[i].boxplot(cluster_data, positions=[cluster + 1], patch_artist=True,
                    boxprops=dict(facecolor=colors[cluster]), widths=0.5)

  axes[i].set_title(feature)
  axes[i].set_xticks(k_med3_df['Cluster_3'].unique() + 1)
  axes[i].set_xticklabels([f'Cluster {cluster}' for cluster in k_med3_df['Cluster_3'].unique()])

plt.tight_layout()

# Create a subplot using boxplots for K-Metiods, clusters = 6

# Create subplots for each feature
fig, axes = plt.subplots(3, 3, figsize=(15, 10))

# Flatten the axes array for easier iteration
axes = axes.flatten()

# Color palette for clusters (using same colors as before)
colors = ['purple', 'yellow', 'darkorchid', 'gold', 'lavender', 'goldenrod']

# Iterate over features and create boxplots
for i, feature in enumerate(df_pca.columns):
  for cluster in k_med6_df['Cluster_6'].unique():
    cluster_data = k_med6_df[k_med6_df['Cluster_6'] == cluster][feature]
    axes[i].boxplot(cluster_data, positions=[cluster + 1], patch_artist=True,
                    boxprops=dict(facecolor=colors[cluster]), widths=0.5)

  axes[i].set_title(feature)
  axes[i].set_xticks(k_med6_df['Cluster_6'].unique() + 1)
  axes[i].set_xticklabels([f'Cluster {cluster}' for cluster in k_med6_df['Cluster_6'].unique()])

plt.tight_layout()

"""The boxplots should look **fantastic.**
Let's visually interpret the 6 cluster boxplots.

Cluster 0:
- Goals Scored: Very low to no goals scored, with a few outliers. This indicates that this cluster likely consists of players who are not primary goal scorers.
- Assists: Low level of assists as well, suggesting that players in this cluster are not significant contributors to scoring opportunities.
- Total Points: Relatively low total points, which aligns with the low goal-scoring and assist values.
- Creativity: Minimal creativity, indicating these players might not be involved heavily in creating scoring opportunities.
- Influence: Low influence scores, meaning these players might not be key figures in the game.
- Clean Sheets: Moderate spread in clean sheets, suggesting some defensive contribution but not outstanding.
- Goals Conceded & Threat: Low to moderate values, indicating a lower level of defensive errors and attacking threat.

Cluster 1:
- Goals Scored: Very low goals scored, with no significant outliers. Players in this cluster seem to have minimal impact on scoring.
- Assists: Similarly low assists, further indicating a lack of offensive contribution.
- Total Points: Low total points, correlating with the low offensive stats.
- Creativity: Very low creativity, suggesting these players aren’t creating many chances.
- Influence: Extremely low influence, likely indicating these players are less central to gameplay.
- Clean Sheets: Lower clean sheet values compared to other clusters.
- Goals Conceded & Threat: Low values, showing limited involvement in both defensive mistakes and attacking plays.

Cluster 2:
- Goals Scored: Moderate to high range in goals scored, with several outliers indicating a mix of high-performing goal scorers.
- Assists: A higher spread in assists, suggesting these players contribute significantly to goal creation.
- Total Points: High total points, reflecting their overall contribution to the game.
- Creativity: High creativity, indicating these players are actively involved in creating scoring opportunities.
- Influence: High influence, suggesting these players are central to the team’s performance.
- Clean Sheets: Decent clean sheets, showing some defensive solidity.
- Goals Conceded & Threat: Moderate to high threat, indicating these players are significant in attacking plays, with a moderate level of defensive involvement.

Cluster 3:
- Goals Scored: High goal-scoring ability, with a wide range and several outliers. Likely consists of top-performing attackers.
- Assists: Highest assists among all clusters, making these players key creators.
- Total Points: High total points, reflecting their overall strong performance.
- Creativity: Very high creativity, indicating a primary role in creating opportunities.
- Influence: High influence scores, suggesting these are key players within their teams.
- Clean Sheets: Moderate clean sheets, showing some defensive contribution but not dominant.
- Goals Conceded & Threat: High threat values, indicating these players are very active in offensive plays.

Cluster 4:
- Goals Scored: Low to moderate goals scored, with a few outliers.
- Assists: Moderate level of assists, indicating a balanced role in the team.
- Total Points: Moderate total points, suggesting consistent but not outstanding performance.
- Creativity: Moderate creativity, showing some involvement in creating plays.
- Influence: Moderate influence, indicating these players have a balanced role without being central figures.
- Clean Sheets: Moderate clean sheets, showing decent defensive performance.
- Goals Conceded & Threat: Moderate spread in both defensive errors and offensive threats, suggesting well-rounded players.

Cluster 5:
- Goals Scored: Low to moderate goals scored, with some spread but generally lower than Clusters 2 and 3.
- Assists: Moderate assists, suggesting a secondary role in goal creation.
- Total Points: Moderate total points, indicating consistent contribution across matches.
- Creativity: Moderate to low creativity, showing these players may not be primary creators.
- Influence: Moderate influence, suggesting a balanced role without being central figures.
- Clean Sheets: Similar to Cluster 4, moderate clean sheets.
- Goals Conceded & Threat: Moderate to low spread, indicating balanced but not outstanding offensive or defensive performance.

Overall Interpretation:
- Clusters 2 and 3: These seem to represent the high-performing players, particularly in offensive roles. Cluster 3 might contain top attackers and creators, while Cluster 2 also includes strong contributors.
- Clusters 0 and 1: These appear to contain lower-performing players, likely those who are not heavily involved in scoring or creating opportunities. Cluster 1 seems to have the least impact overall.
- Clusters 4 and 5: These are more balanced clusters with moderate performance across features. They might represent versatile or secondary players who contribute both offensively and defensively but aren’t standout performers.

Actionable Insights:
- Focus on Clusters 2 and 3 for selecting key players, while Clusters 0 and 1 could indicate areas for improvement or specific roles within the team.

**It's interesting how the analysis naturally leads back to the 3-cluster solution as potentially the most meaningful configuration. This observation highlights a few important points:**

Why 3 Clusters Might Be Ideal:
Balance Between Detail and Simplicity:
- With 3 clusters, you capture significant distinctions among different groups without overcomplicating the segmentation. This balance often provides a clearer, more actionable view of the data.
Clearer Interpretation:
- The 3-cluster configuration tends to highlight the major differences in player performance—such as top performers, average performers, and low performers—making it easier to draw conclusions and make decisions.
Consistent Findings:
- The fact that both the boxplots and other analyses (like silhouette scores or variance explained by PCA) seem to favor 3 clusters suggests that this configuration best reflects the underlying structure of the data.
"""

# Calculate Silhouette Scores for K-Means

# Import KMeans
from sklearn.cluster import KMeans

# Assuming you have already fitted KMeans models with different cluster numbers (e.g., kmeans2, kmeans3, kmeans6) and stored the labels in your DataFrames

#Fit the KMeans model with 2 clusters
kmeans2 = KMeans(n_clusters=2, random_state = 123)
kmeans2.fit(df_pca)

# Calculate Silhouette Score for 2 clusters (K-Means)
kmeans_sil_score_2 = silhouette_score(df_pca, kmeans2.labels_)
print(f"Silhouette Score for K-Means with 2 clusters: {kmeans_sil_score_2}")

#Fit the KMeans model with 3 clusters
kmeans3 = KMeans(n_clusters=3, random_state = 123)
kmeans3.fit(df_pca)

# Calculate Silhouette Score for 3 clusters (K-Means)
kmeans_sil_score_3 = silhouette_score(df_pca, kmeans3.labels_)
print(f"Silhouette Score for K-Means with 3 clusters: {kmeans_sil_score_3}")

#Fit the KMeans model with 6 clusters
kmeans6 = KMeans(n_clusters=6, random_state = 123)
kmeans6.fit(df_pca)

# Calculate Silhouette Score for 6 clusters (K-Means)
kmeans_sil_score_6 = silhouette_score(df_pca, kmeans6.labels_)
print(f"Silhouette Score for K-Means with 6 clusters: {kmeans_sil_score_6}")


# Calculate Silhouette Scores for K-Medoids

# Assuming you have already fitted KMedoids models with different cluster numbers (e.g., kmedoids2, kmedoids3, kmedoids6) and stored the labels in your DataFrames

# Calculate Silhouette Score for 2 clusters (K-Medoids)
kmedoids_sil_score_2 = silhouette_score(df_pca, kmedoids2.labels_)
print(f"Silhouette Score for K-Medoids with 2 clusters: {kmedoids_sil_score_2}")

# Calculate Silhouette Score for 3 clusters (K-Medoids)
kmedoids_sil_score_3 = silhouette_score(df_pca, kmedoids3.labels_)
print(f"Silhouette Score for K-Medoids with 3 clusters: {kmedoids_sil_score_3}")

# Calculate Silhouette Score for 6 clusters (K-Medoids)
kmedoids_sil_score_6 = silhouette_score(df_pca, kmedoids6.labels_)
print(f"Silhouette Score for K-Medoids with 6 clusters: {kmedoids_sil_score_6}")

"""Silhouette Score Interpretation:
- Silhouette Score Range:
  - The silhouette score ranges from -1 to 1.
  - A score closer to 1 indicates that the data points are well-clustered, with distinct separation between clusters.
  - A score near 0 indicates that clusters are not well separated, and a negative score suggests that data points may have been assigned to the wrong clusters.

K-Means Clustering:
- 2 Clusters:
  - 0.476
  - The highest silhouette score for K-Means.
  - The 2-cluster solution offers the most well-separated clusters.
- 3 Clusters:
  - 0.459
  - The score is slightly lower than the 2-cluster solution.
  - The third cluster doesn’t significantly improve the separation of clusters.
- 6 Clusters:
  - 0.400
  - The silhouette score drops more noticeably here.
  - The 6-cluster solution may over-segment the data, leading to less distinct clusters.

K-Medoids Clustering:
- 2 Clusters:
  - 0.460
  - The highest silhouette score for K-Medoids, similar to K-Means.
  - The 2-cluster solution is the most well-defined for this method as well.
- 3 Clusters:
  - 0.314
  - A significant drop in the silhouette score compared to the 2-cluster solution
  - The 3-cluster configuration may not be as well-separated in K-Medoids as it is in K-Means.
- 6 Clusters:
  - 0.286:
  - The lowest silhouette score among all configurations.
  - While the 6-cluster solution results in poor separation between clusters for K-Medoids, it may offer a domain expert valuable insight.

Key Observations:
- 2 Clusters:
  - Both K-Means and K-Medoids perform best with 2 clusters.
  - This configuration provides the most distinct separation between groups.
- 3 Clusters:
  - K-Means maintains a relatively decent silhouette score with 3 clusters.
  - K-Medoids shows a more significant drop.
  - K-Medoids may not be capturing distinct groups as effectively with 3 clusters.
- 6 Clusters:
  - Both methods show the lowest silhouette scores with 6 clusters.
  - 6 clusters provides very granular data that may be unnecessarily complicated, depending on the required interpretation.

Implications:
Given the highest silhouette scores, the 2-cluster solution seems to be the most robust for both methods, suggesting that the data might naturally group into two main categories.

**Recommendation:**
- Focus on 2 Clusters: Given the silhouette scores, the 2-cluster solution is likely the most meaningful and well-separated configuration. It might be worth focusing on this for any practical applications or further analysis.
- Consider 3 Clusters with K-Means: If the 3-cluster solution offers more meaningful insights (e.g., based on domain knowledge or specific use cases), K-Means might be a better choice than K-Medoids due to its relatively higher silhouette score.
"""

# Print the shape of df_pca and cluster centers for each kmedoids model
print(f"Shape of df_pca: {df_pca.shape}")
print(f"Shape of kmedoids2.cluster_centers_: {kmedoids2.cluster_centers_.shape}")
print(f"Shape of kmedoids3.cluster_centers_: {kmedoids3.cluster_centers_.shape}")
print(f"Shape of kmedoids6.cluster_centers_: {kmedoids6.cluster_centers_.shape}")

"""## **Hierarchical Clustering**

- Hierarchical clustering is a popular unsupervised learning algorithm used for grouping similar data points into clusters based on the hierarchical structure of the data. The algorithm works by recursively merging the closest data points or clusters until all the data points belong to a single cluster.

- There are two main types of hierarchical clustering: agglomerative and divisive. Agglomerative clustering starts with each data point as a separate cluster and recursively merges the closest clusters until all the data points belong to a single cluster. Divisive clustering, on the other hand, starts with all the data points in a single cluster and recursively splits the clusters until each data point belongs to a separate cluster. Here, we will implement the agglomerative clustering.

- The steps involved in agglomerative clustering are as follows:
    - Assign each data point to a separate cluster.
    - Compute the dissimilarity between each pair of clusters.
    - Merge the two closest clusters into a single cluster.
    - Update the dissimilarity between the new cluster and the remaining clusters.
    - Repeat steps 3 and 4 until all the data points belong to a single cluster.

- Agglomerative clustering can be used with different linkage criteria to compute the dissimilarity between clusters. The most common linkage criteria are:
    - Single linkage: The dissimilarity between two clusters is the distance between the closest two data points in the clusters.
    - Complete linkage: The dissimilarity between two clusters is the distance between the farthest two data points in the clusters.
    - Average linkage: The dissimilarity between two clusters is the average distance between all the data point pairs in the clusters.

Agglomerative clustering can be computationally expensive for large datasets, as it requires computing the pairwise dissimilarities between all the data points. However, it can be useful for datasets where the underlying structure is hierarchical or where the number of clusters is not known a priori.
"""

data_pca.head()

hc_df = data_pca.copy()

hc_df1 = hc_df.copy()

from scipy.cluster.hierarchy import linkage, cophenet
from scipy.spatial.distance import pdist

# Filter out non-numeric columns from hc_df1
hc_df1_numeric = hc_df1.select_dtypes(include=[np.number])

# List of distance metrics
distance_metrics = ["euclidean", "chebyshev", "mahalanobis", "cityblock"]

# List of linkage methods
linkage_methods = ["single", "complete", "average", "weighted"]

# Initialize variables to store the highest cophenetic correlation and corresponding methods
high_cophenet_corr = 0
high_dm_lm = [0, 0]

# Loop through each combination of distance metric and linkage method
for dm in distance_metrics:
    for lm in linkage_methods:
        Z = linkage(hc_df1_numeric, metric=dm, method=lm)
        c, coph_dists = cophenet(Z, pdist(hc_df1_numeric))
        print("Cophenetic correlation for {} distance and {} linkage is {}.".format(
                dm.capitalize(), lm, c))

        # Check if the current combination has the highest cophenetic correlation
        if high_cophenet_corr < c:
            high_cophenet_corr = c
            high_dm_lm[0] = dm
            high_dm_lm[1] = lm

# Print the best combination of distance metric and linkage method
print('*' * 100)
print("Highest cophenetic correlation is {}, which is obtained with {} distance and {} linkage.".format(
        high_cophenet_corr, high_dm_lm[0].capitalize(), high_dm_lm[1]))

"""This code is designed to evaluate different combinations of distance metrics and linkage methods in hierarchical clustering to find the combination that yields the highest cophenetic correlation coefficient. The cophenetic correlation coefficient measures how well a dendrogram preserves the pairwise distances between the original data points.
- linkage: This function performs hierarchical/agglomerative clustering.
- cophenet: This function calculates the cophenetic correlation coefficient.
- pdist: This function computes the pairwise distance matrix between observations.
- hc_df1.select_dtypes(include=[np.number]): This line selects only the numeric columns from hc_df1, which is necessary because hierarchical clustering requires numeric data.
- distance_metrics: A list of different distance metrics to be evaluated.
- linkage_methods: A list of different linkage methods to be evaluated.
- high_cophenet_corr: This variable stores the highest cophenetic correlation coefficient found during the loop.
- high_dm_lm: A list that stores the distance metric and linkage method corresponding to the highest cophenetic correlation coefficient.

**Purpose:**
-	Objective: The goal is to determine which combination of distance metric and linkage method produces **the most faithful** hierarchical clustering solution in terms of preserving the original data’s pairwise distances.
- Result: The combination with the highest cophenetic correlation coefficient is considered the best for clustering this particular dataset.

Practical Application:
- After determining the best combination, you can use it to perform hierarchical clustering and generate a dendrogram or other cluster analyses, **confident** that the method you’ve chosen best represents the data’s structure.

The cophenetic correlation coefficient is a measure used in hierarchical clustering to assess how well the resulting dendrogram (a tree-like diagram showing the arrangement of the clusters produced by hierarchical clustering) preserves the original pairwise distances between the data points.

What It Measures:
- Original Pairwise Distances: These are the distances between every pair of data points in the original dataset before clustering.
- Cophenetic Distances: After hierarchical clustering, the cophenetic distance between two data points is defined as the height at which the two points are first merged into the same cluster in the dendrogram.

The cophenetic correlation coefficient compares the original pairwise distances with the cophenetic distances from the dendrogram. It is calculated as the Pearson correlation coefficient between these two sets of distances.

Why It’s Important:
- High Cophenetic Correlation: A high cophenetic correlation coefficient (close to 1) indicates that the clustering algorithm has preserved the original distances well, meaning the structure of the data is faithfully represented in the dendrogram.
- Low Cophenetic Correlation: A low cophenetic correlation coefficient (closer to 0 or negative) suggests that the dendrogram does not accurately reflect the original data’s structure, meaning the clustering might not be very reliable.

**The cophenetic correlation coefficient is a key metric in hierarchical clustering, providing insight into how well the clustering process has preserved the original data’s structure.**
It helps you determine the most appropriate clustering method by comparing the clustering solution to the actual distances in the dataset.
"""

import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage

# hc_df1_numeric is your dataset after removing non-numeric columns

# Perform hierarchical clustering using the best combination (Euclidean distance, average linkage)
Z = linkage(hc_df1_numeric, metric='euclidean', method='average')

# Plot the dendrogram
plt.figure(figsize=(20, 7))
dendrogram(Z)
plt.title('Hierarchical Clustering Dendrogram (Euclidean Distance, Average Linkage)')
plt.xlabel('Sample Index')
plt.ylabel('Distance')

"""**Observing Your Dendrogram:**
- A noticeable large gap occurs around the 6 to 7 distance mark. Cutting the dendrogram at this level will result in a relatively small number of distinct clusters. There is also another possible cut around the 4 distance mark, which would yield more clusters, but they would be less distinct from each other compared to the first option.

- Suggested Number of Clusters:
 - Cut at a Distance of ~6:
    - This will likely give you 2 or 3 large clusters, as there are fewer major vertical merges around this distance.
    - This is suitable if you’re looking for very distinct groupings in your data.
  - Cut at a Distance of ~4:
    - This will likely result in 5 to 6 clusters.
    - This is ideal if you want more granularity in your clusters but still want them to be distinct enough.

If you’re looking for broader, more separated clusters, the cut at 6 would make sense.

If you’re aiming for more detailed sub-clusters, the cut at 4 would be more appropriate.

"""

from scipy.cluster.hierarchy import fcluster

# Cut the dendrogram at a specific height (e.g., 5) to form flat clusters
max_d = 5
clusters = fcluster(Z, max_d, criterion='distance')

# Add the cluster labels to your dataframe
hc_df1_numeric['Cluster'] = clusters

# Display the first few rows to verify
print(hc_df1_numeric.head())

# This scatterplot shows 5 clusters from the Dendrogram
plt.figure(figsize=(10, 7))
sns.scatterplot(x=data_pca['PC1'], y=data_pca['PC2'], hue=hc_df1_numeric['Cluster'], palette='viridis', s=100)
plt.title('Hierarchical Clustering with Dendrogram Cut')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')

"""**Interpretation:**
Dimensionality Reduction: The two principal components explain a significant portion of the variance in the data, and the separation of clusters in this reduced space suggests the clusters represent distinct groupings within the original high-dimensional data.

- Cluster 1 and Cluster 5:
  - These clusters seem to contain data points that are quite distinct from the others. Cluster 5, with only a few points, could represent outliers.
- Clusters 2, 3, and 4:
  - These clusters are more densely packed and might represent data points with more subtle differences between them.
"""

from sklearn.cluster import AgglomerativeClustering

# Perform Agglomerative Clustering
agg_clustering = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='average')
clusters = agg_clustering.fit_predict(hc_df1_numeric)

# Add the cluster labels to your dataframe
hc_df1_numeric['Cluster'] = clusters

# This scatterplot shows 3 clusters from the Dendrogram
plt.figure(figsize=(10, 7))
sns.scatterplot(x=data_pca['PC1'], y=data_pca['PC2'], hue=hc_df1_numeric['Cluster'], palette='viridis', s=100)
plt.title('Agglomerative Clustering Results')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')

"""**Key Observations:**

Clusters: There are 3 distinct clusters (labeled as 0, 1, and 2) based on the colors in the plot:
- Cluster 0 (dark purple):
  - This cluster represents the majority of the points, densely packed in the lower-left region of the plot. Most of the data falls into this cluster.
- Cluster 1 (teal/blue):
  - A small group of points in the upper-right region of the plot, showing a more distinct separation from the main body of Cluster 0.
- Cluster 2 (yellow):
  - This cluster consists of only one point in the lower-right section of the plot, possibly representing an outlier.

Separation of Clusters:
- Cluster 0 (purple) occupies a broad range of values in Principal Component 1 and Principal Component 2, but it’s mostly concentrated between -2 and 2 on both axes.
- Cluster 1 (teal) is well-separated from Cluster 0, with points in higher ranges of Principal Component 1 (from 4 to 10). This suggests that these points differ significantly from Cluster 0, which explains their separation.
- Cluster 2 (yellow) contains a single outlying point, suggesting that this data point may be an anomaly or significantly different from the others.

Cluster Density:
- Cluster 0 is much denser, indicating that most of the data points share similar characteristics in the lower region of the plot.
- Cluster 1 is sparse, representing only a few data points that are similar to each other but distinct from the main cluster.
- Cluster 2 appears to represent an outlier that doesn’t fit well into any of the main groupings.

Insights:
- Cluster 1 may represent a unique group of data points that differ significantly from the majority of the data (Cluster 0). This group could be of special interest for further analysis.
- Cluster 2 (yellow) likely represents an outlier, and further investigation may be necessary to determine whether it’s truly an anomaly or if it has important implications.
- Cluster 0 contains most of the data points, indicating that they share similar characteristics in terms of their principal components.

**Ward Linkage:**

The Ward method minimizes the variance within each cluster. It is known to create clusters of relatively equal sizes and works particularly well when the data is continuous and the clusters are expected to be spherical.
- Number of Clusters: You can change the n_clusters parameter to experiment with different numbers of clusters.
"""

from sklearn.cluster import AgglomerativeClustering

# Perform Agglomerative Clustering with Ward linkage
agg_clustering_ward = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')
clusters_ward = agg_clustering_ward.fit_predict(hc_df1_numeric)

# Add the cluster labels to your dataframe
hc_df1_numeric['Cluster_Ward'] = clusters_ward

# Plot the clusters using Ward linkage on the first two principal components
plt.figure(figsize=(10, 7))
sns.scatterplot(x=data_pca['PC1'], y=data_pca['PC2'], hue=hc_df1_numeric['Cluster_Ward'], palette='viridis', s=100)
plt.title('Agglomerative Clustering Results with Ward Linkage')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')

# Define the custom color palette
custom_palette = ['purple', 'yellow', 'lavender']

# Create a subplot grid to hold the boxplots for each feature
fig, axes = plt.subplots(3, 4, figsize=(20, 20))
counter = 0

# Iterate over the axes to create boxplots for each feature against the clusters
for ii in range(3):
    for jj in range(4):
        if counter < len(hc_df1_numeric.columns) - 1:  # Adjust for the number of features
            sns.boxplot(
                ax=axes[ii][jj],
                data=hc_df1_numeric,
                y=hc_df1_numeric.columns[counter],
                x='Cluster_Ward',  # Assuming 'Cluster_Ward' is the column with cluster labels
                palette=custom_palette)
            counter += 1

# Adjust layout for better readability
fig.tight_layout(pad=3.0)

"""**General Interpretation:**
- Box Plot Elements:
  - Boxes represent the interquartile range (IQR) where 50% of the data lies (from the 25th percentile to the 75th percentile).
  - Whiskers show the range of the data, excluding outliers.
  - Outliers are data points outside the whiskers, marked with individual dots.
  - Median Line: The horizontal line inside each box represents the median (50th percentile).

Key Observations:

Cluster 0 (purple):
- Tends to have lower values in most principal components (e.g., PC1, PC2, PC3, PC4, PC5), indicating that this cluster has data points with characteristics that differ from other clusters, primarily residing in the lower end of the PC distribution.
- There’s a significant amount of variance in PC1, with a broader range and more outliers compared to other components.
- The interquartile range in most components is more condensed, with relatively fewer extreme values compared to Cluster 1 and Cluster 2.

Cluster 1 (yellow):
- This cluster shows relatively neutral or centered values in many of the principal components. For instance, in PC1, PC2, and PC4, the data points are clustered close to the zero point.
- This cluster is smaller and shows less variability overall compared to the other clusters. Its distribution is often quite tight, and its IQR is smaller, indicating less spread.
- Interestingly, Cluster 1 often shows up as a middle ground between Cluster 0 and Cluster 2.

Cluster 2 (light blue):
- This cluster generally has higher values in the principal components. For example, PC1, PC2, PC3, and PC4 show this cluster occupying higher ranges compared to Cluster 0.
- Cluster 2 tends to have more extreme values and wider ranges, particularly in PC1 and PC2, where it extends to larger positive values.
- In many components, the variance is higher, showing a broader distribution of data points.

Cluster-Specific Insights:
- Cluster 0 (purple) might represent data points with generally lower values across most principal components, possibly indicating that these data points share a common characteristic, such as lower scores or different patterns in the data.
- Cluster 1 (yellow) seems to be the smallest and most homogeneous cluster. It could represent a specialized subset of data points that are less variable and closer to the overall mean.
- Cluster 2 (light blue) has a broader distribution and higher values across multiple components, suggesting that it contains more diverse or extreme data points that stand out compared to the rest of the dataset.

## **Gaussian Mixture Model (GMM) Clustering**

- Gaussian Mixture Model (GMM) clustering is a probabilistic clustering method that models the data as a mixture of Gaussian distributions. The algorithm works by estimating the parameters of the Gaussian distributions that best fit the data and then assigning each data point to the Gaussian distribution with the highest probability.

- The steps involved in GMM clustering are as follows:
    - Initialize the parameters of the Gaussian distributions, which include the means, covariances, and mixing coefficients.
    - Compute the probability density function of each data point under each Gaussian distribution.
    - Assign each data point to the Gaussian distribution with the highest probability.
    - Update the parameters of the Gaussian distributions based on the data points assigned to them.
    - Repeat steps 2-4 until the parameters converge.

GMM clustering can be used to identify clusters with arbitrary shapes and sizes, as the Gaussian distributions can model different shapes and orientations. Additionally, GMM clustering can estimate the uncertainty of each data point's assignment to a cluster, which can be useful in applications where the data is noisy or ambiguous.
"""

# Create a copy of the DataFrame containing only numerical features for GMM clustering
gmm_df = data_pca.select_dtypes(include=[np.number]).copy()

# Perform GMM clustering on the copied DataFrame
gmm = GaussianMixture(n_components=3, random_state=1)  # Adjust n_components as needed
gmm_labels = gmm.fit_predict(gmm_df)

# Add the GMM cluster labels to the DataFrame
gmm_df['GMM_Cluster'] = gmm_labels

"""**Scatter Plot with Cluster Labels:**
- Purpose: Visualize how data points are distributed across the clusters identified by GMM.
- Tool: matplotlib and seaborn
- Best for: Visualizing 2D data, especially when you have reduced the dimensionality of your data (e.g., using PCA).
"""

# Create Scatter Plot with Cluster Labels
plt.figure(figsize=(10, 7))
sns.scatterplot(x=gmm_df['PC1'], y=gmm_df['PC2'], hue=gmm_df['GMM_Cluster'], palette='viridis', s=100)
plt.title('GMM Clustering Results')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')

"""**Interpretation of the Clusters:**
- Cluster 0 (Purple):
 - This is the largest cluster and is spread out across a broad range of values for Principal Component 1 (PC1).
 - It dominates the right side of the plot and includes data points that are more dispersed along the PC1 axis.
 - This suggests that the features contributing to PC1 have more variation within this cluster, and the members of this cluster have diverse characteristics.
- Cluster 1 (Teal/Green):
 - Cluster 1 is tightly packed on the left side of the plot, particularly in the lower range of PC1.
 - The compactness of this cluster suggests that the data points here are more similar to each other in terms of the features that PC1 represents.
 - The tight clustering indicates a strong commonality in the characteristics of the points within this group.
 - Cluster 2 (Yellow):
- Cluster 2 is situated between Cluster 0 and Cluster 1 along the PC1 axis but slightly higher along the PC2 axis.
 - This cluster shows a moderate spread, indicating that the data points are somewhat similar but with more variance compared to Cluster 1.
 - The positioning of this cluster suggests that it represents a group with intermediate characteristics—distinct from both Cluster 0 and Cluster 1.

**Overall Insights:**
- The clusters identified by the GMM suggest that there are three main groups within the data, with varying degrees of similarity.
- Cluster 0 is the most diverse group, Cluster 1 is the most homogeneous, and Cluster 2 has characteristics that lie somewhere between the other two.
- The spread and positioning of these clusters give insights into the underlying structure of the data, particularly in how the features captured by the principal components differentiate these groups.

---


###**Pair Plot:**
- Purpose: Examine pairwise relationships between features, colored by cluster assignment.
- Tool: seaborn
- Best for: Analyzing relationships across multiple dimensions and seeing how clusters are separated.
"""

sns.pairplot(gmm_df, hue='GMM_Cluster', palette='viridis')
plt.suptitle('GMM Clustering Pair Plot', y=1.02)

"""The pair plot offers a comprehensive view of the relationships between the different features within the dataset, with clusters identified by the Gaussian Mixture Model (GMM).

**Insights:**

Cluster Distribution Across Principal Components:
- The diagonal plots show the distribution of each principal component (PC1, PC2, etc.) within each cluster.
 - Cluster 0 (purple) seems to have the widest distribution across most principal components, indicating greater variability within this cluster.
 - Clusters 1 (teal) and 2 (yellow) are more tightly clustered in certain dimensions, which suggests that the members of these clusters are more similar to each other in those specific features.
  - Cluster 1 (teal) tends to be concentrated in lower ranges of the principal components, particularly PC1 and PC2, indicating that these data points have lower values in the features that contribute most to these components.

Relationships Between Components:
- The scatter plots in the off-diagonal cells show how the clusters are distributed across pairs of principal components. For example, in the scatter plots involving PC1 and PC2, you can observe that Cluster 0 (purple) spans a large portion of the space, indicating a wide variety of data points. Cluster 1 (teal) is tightly grouped towards the lower left, and Cluster 2 (yellow) occupies a distinct space, somewhat overlapping with Cluster 0 but maintaining its own region.
- These relationships suggest that while there is some overlap between clusters, particularly between Cluster 0 and Cluster 2, the clusters are generally well-separated, especially in the principal components that explain the most variance (e.g., PC1 and PC2).

Univariate Distributions Within Clusters:
- The density plots along the diagonal for each principal component show how data is distributed within each cluster.
  - Cluster 0 generally shows broader, flatter distributions, while Clusters 1 and 2 have sharper peaks, indicating more homogeneity.
- The univariate distribution of Cluster 2 (yellow) seems to be more distinct in certain principal components, which might indicate that this cluster represents a more specialized group within your data.

Potential Overlap:
- There are indications of overlap between clusters in some principal component pairings, which might suggest areas where the GMM clustering could be capturing complex relationships or where data points are not easily separable.

Cluster Characteristics:
- Each cluster seems to capture different characteristics within the data.
 - Cluster 0 may represent a more diverse group, with a wide range of values across multiple dimensions.
 - Cluster 1 is more homogeneous and potentially represents a lower-value segment in terms of the principal components.
 - Cluster 2, while somewhat overlapping with Cluster 0, might capture a middle ground or a specialized segment within the data.

"""

# Calculate the mean and standard deviation of each feature by cluster
cluster_profile = gmm_df.groupby('GMM_Cluster').agg(['mean', 'std'])

# Display the cluster profiles
print(cluster_profile)

# Optional: Visualize the cluster profiles using bar plots or radar charts
for feature in gmm_df.columns[:-1]:  # Exclude the cluster column
    plt.figure(figsize=(8, 5))
    sns.barplot(x='GMM_Cluster', y=feature, data=gmm_df, ci='sd', palette='viridis')
    plt.title(f'{feature} by Cluster')
    plt.ylabel('Mean Value')

"""**Interpretation:**
- PC1 seems to be a critical component for differentiating the clusters:
  - Cluster 0 has high positive values for PC1, indicating that the data points in this cluster are quite distinct from the other clusters in terms of the first principal component.
  - Cluster 1 is characterized by negative values for PC1, showing a strong separation from Cluster 0.
  - Cluster 2 is centered around zero for PC1, indicating that it may be neutral or closer to the average for this component, with little variability.
  - Cluster Separation: The separation between clusters is clear in this plot. Cluster 0 occupies the positive range, Cluster 1 the negative range, and Cluster 2 stays around zero.

- The fact that all of GMM_Cluster 1 is classified into Cluster_2 indicates a strong alignment between these two clusters. In contrast, GMM_Cluster 2 has some points that belong to Cluster_2, but the alignment is not as strong.
- There are no points from Cluster_2 that are associated with GMM_Cluster 0, indicating that this cluster has no overlap with Cluster_2.
- GMM_Cluster 2 has a very strong alignment with Cluster_3, as all points in GMM_Cluster 2 are classified into Cluster_3 with no variability.
- GMM_Cluster 0 and GMM_Cluster 1 show moderate associations with Cluster_3, but only around half of the points in these clusters are classified into Cluster_3.
- There is some variability in GMM_Cluster 0, which suggests that points in this cluster may be classified into multiple clusters, including Cluster_3.

---


###**Box Plot:**
- Purpose: Compare the distribution of numerical features across different clusters.
- Tool: seaborn
- Best for: Identifying which features contribute most to the differences between clusters.
"""

fig, axes = plt.subplots(3, 4, figsize=(20, 20))
counter = 0
for i in range(3):
    for j in range(4):
        if counter < len(gmm_df.columns) - 1:  # Adjust based on number of features
            sns.boxplot(ax=axes[i][j], x='GMM_Cluster', y=gmm_df.columns[counter], data=gmm_df, palette='viridis')
            counter += 1
fig.tight_layout(pad=3.0)

"""**Interpretation of Principal Components (PC1 to PC10):**

Each of the principal component plots compares the distribution of the principal component scores across the GMM_Clusters:
- PC1:
	- Cluster 0 has higher values for PC1, with a large range of variability.
	- Cluster 1 shows lower values, clustered around -2.
	- Cluster 2 has values closer to 0, with minimal variation compared to Cluster 0.

- PC2:
  - Cluster 0 has the highest range, both positive and negative, indicating large variation in the values.
  - Cluster 1 shows positive values clustered tightly around 0.5, with limited variability.
  - Cluster 2 also shows moderate positive values, slightly lower than Cluster.
-PC3:
  - Cluster 0 shows a wide spread of values from -2 to 2.
  - Cluster 1 has a concentrated range of negative values, around -1.
  - Cluster 2 displays positive values, with a distribution centered around 0.5.
- PC4:
  - Cluster 0 has negative values and a broader range.
  - Cluster 1 clusters around 0, with less variability.
  - Cluster 2 shows positive values, with more spread than Cluster 1.
- PC5 to PC10:
  - These components show similar trends, with Cluster 0 having higher or broader ranges in many cases, while Cluster 1 tends to have more tightly packed distributions centered around negative or neutral values.
  - Cluster 2 generally shows positive values with moderate ranges.

Interpretation of Clusters:
- Cluster_2:
  - Cluster 2 in GMM_Cluster 2 is fully associated with Cluster_2 (represented as a solid bar). This indicates that GMM_Cluster 2 is almost entirely comprised of points classified into Cluster_2.
  - There are no points from Cluster 0 or Cluster 1 that belong to Cluster_2.
- Cluster_3:
  - Cluster 3 is predominantly associated with GMM_Cluster 2 (as seen from the tall green bar in the rightmost plot).
  - GMM_Cluster 0 and GMM_Cluster 1 show smaller associations with Cluster_3, indicating partial overlap.

Overall Interpretation:
- Cluster 0: Exhibits wide variability across several principal components, with values typically in the positive range. This suggests that GMM_Cluster 0 may represent data points with broader, more extreme characteristics across multiple components.
- Cluster 1: Generally shows negative or near-zero values across most components, with minimal variation. This suggests that GMM_Cluster 1 captures more homogeneous points that have moderate or lower values in principal components.
- Cluster 2: Tends to have more neutral or positive values with moderate variability. This cluster shows clear alignment with Cluster_3 and significant overlap with Cluster_2, suggesting that it captures a specific subset of data points associated with these clusters.

Next Steps:
- Analyze Feature Contribution: Understanding which original features contribute to the differences in principal components can further explain why GMM_Cluster 0 and GMM_Cluster 2 exhibit such broad variability.
- Investigate Overlapping Clusters: The overlap between GMM_Clusters and Cluster_2 and Cluster_3 suggests that further investigation into what defines these clusters could reveal deeper insights.

## **Density-Based Spatial Clustering of Applications with Noise (DBSCAN)**

- DBSCAN is a clustering algorithm that groups together points that are closely packed together while marking points that are not part of any cluster as noise. The algorithm works by defining a neighborhood around each point and then determining whether **the point is a core point, a border point, or a noise point** based on the density of points within the neighborhood.

- The steps involved in DBSCAN are as follows:
    - Define a neighborhood around each point using a distance metric and a radius.
    - Identify the core points as those that have at least a minimum number of points within their neighborhood.
    - Connect the core points to their directly reachable neighbors to form clusters.
    - Identify the border points as those that are not core points but belong to a cluster.
    - Assign the noise points to their own separate cluster.

DBSCAN can be useful for datasets with irregular shapes or where the number of clusters is not known a priori. It can also handle noisy data by assigning it to its own separate cluster. Additionally, DBSCAN does not require the specification of the number of clusters or assumptions about the shape of the clusters.

It is a very powerful algorithm for finding high-density clusters, but the problem is determining the best set of hyperparameters to use with it. It includes two hyperparameters, eps, and min samples. Since it is an unsupervised algorithm, you have no control over it, unlike a supervised learning algorithm, which allows you to test your algorithm on a validation set. The approach we can follow is trying out a bunch of different combinations of values and finding the silhouette score for each of them.
"""

# Copy the DataFrame with only numerical features
dbscan_df = data_pca.select_dtypes(include=[np.number]).copy()

# Create a second copy, if needed for further processing
dbscan_df1 = dbscan_df.copy()

scaler = StandardScaler()
scaled_dbscan_df = scaler.fit_transform(dbscan_df)

# Define the DBSCAN model with desired parameters
dbscan = DBSCAN(eps=0.5, min_samples=5)  # You may need to tune these parameters

# Fit the model to the data
dbscan_labels = dbscan.fit_predict(scaled_dbscan_df)

# Add the labels to your DataFrame
dbscan_df1['DBSCAN_Cluster'] = dbscan_labels

# Plot the clusters
plt.figure(figsize=(10, 7))
sns.scatterplot(x=dbscan_df['PC1'], y=dbscan_df['PC2'], hue=dbscan_df1['DBSCAN_Cluster'], palette='viridis')
plt.title('DBSCAN Clustering Results')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend(title='Cluster')

db_cluster_profile = dbscan_df1.groupby('DBSCAN_Cluster').mean()

# Highlight the maximum values in each column
db_cluster_profile.style.highlight_max(color = "lightgreen", axis = 0)

"""###**Optimize DBSCAN**"""

from sklearn.model_selection import ParameterGrid

# Define parameter grid for DBSCAN
param_grid = {'eps': [0.5, 1, 1.5, 2, 2.5, 3, 4, 5],
    'min_samples': [3, 5, 10, 15, 20, 25, 30]}

# Initialize variables for storing the best parameters and silhouette score
best_score = -1
best_params = None

# Iterate over all combinations of the parameter grid
for params in ParameterGrid(param_grid):
    dbscan = DBSCAN(**params)
    labels = dbscan.fit_predict(dbscan_df)

    # Ensure there is more than one cluster (excluding noise labeled as -1)
    if len(set(labels)) > 1:
        score = silhouette_score(dbscan_df, labels, metric='euclidean')
        print(f"Parameters: {params}, Silhouette Score: {score:.4f}")

        # Update best parameters if the current score is better
        if score > best_score:
            best_score = score
            best_params = params

# Apply DBSCAN with the best found parameters
if best_params is not None:
    print(f"\nBest Parameters: {best_params}, Best Silhouette Score: {best_score:.4f}")
    best_dbscan = DBSCAN(**best_params)
    dbscan_df['db_segments'] = best_dbscan.fit_predict(dbscan_df)
else:
    print("No suitable parameters found.")

# Run DBscan with best parameters

best_dbscan = DBSCAN(**best_params)
dbscan_df['db_segments'] = best_dbscan.fit_predict(dbscan_df)

plt.figure(figsize=(10, 7))
sns.scatterplot(x=dbscan_df['PC1'], y=dbscan_df['PC2'], hue=dbscan_df['db_segments'], palette='viridis')
plt.title('DBSCAN Clustering Results with Best Parameters')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend(title='Cluster')

"""### **Choosing the Best Algorithm**

- **We can compute the silhouette score to choose the best algorithm among all the algorithms.**
"""

# Fit hierarchical clustering model
hc = AgglomerativeClustering() # You may want to define the number of clusters and other parameters here
hc.fit(dbscan_df) # Fit the model to your data (dbscan_df)

labels = hc.labels_ # Get the cluster labels
silhouette_avg_hc = silhouette_score(dbscan_df, labels, metric='euclidean')

# Fit Agglomerative Clustering model
agg = AgglomerativeClustering() # You may want to define the number of clusters and other parameters here
agg.fit(dbscan_df) # Fit the model to your data (dbscan_df)

labels = agg.labels_ # Get the cluster labels
silhouette_avg_agg = silhouette_score(dbscan_df, labels, metric='euclidean') #Calculate and store the score

from sklearn.mixture import GaussianMixture


# Fit Gaussian Mixture Model
gmm = GaussianMixture(n_components=5) # Example with 5 components, adjust as needed
gmm.fit(dbscan_df)

labels = gmm.predict(dbscan_df)
silhouette_avg_gmm = silhouette_score(dbscan_df, labels, metric='euclidean')

print(f"Silhouette Score for K-Means with 2 clusters: {kmeans_sil_score_2}")
print(f"Silhouette Score for K-Means with 3 clusters: {kmeans_sil_score_3}")
print(f"Silhouette Score for K-Means with 6 clusters: {kmeans_sil_score_6}")
print(f"Silhouette Score for K-Medoids with 2 clusters: {kmedoids_sil_score_2}")
print(f"Silhouette Score for K-Medoids with 3 clusters: {kmedoids_sil_score_3}")
print(f"Silhouette Score for K-Medoids with 6 clusters: {kmedoids_sil_score_6}")
print(f"Silhouette Score for Hierarchical Clustering: {silhouette_avg_hc:.4f}")
print(f"Silhouette Score for Agglomerative Clustering: {silhouette_avg_agg:.4f}")
print(f"Silhouette Score for GMM Clustering: {silhouette_avg_gmm:.4f}")
print(f"Silhouette Score for DBSCAN: {best_score:.4f}")

"""**Based on the silhouette score, we can see the optimized DBScan algorithm gives the best score on the data.**"""

# Create a copy of the original dataframe with the cluster number assigned to each row

# Create a copy of the original data
final_df = data.copy()

# Add the DBSCAN cluster labels from the 'dbscan_df' to the original data
final_df['DBSCAN_Cluster'] = dbscan_df['db_segments']

# Display the first few rows to verify the new feature has been added
print(final_df.head())

print (final_df.shape)

final_df.head(1)

final_df['DBSCAN_Cluster'].unique()

print('player names in cluster -1')

final_df[final_df['DBSCAN_Cluster'] == -1]['Player_Name']

"""This worksheet offers you many robust tools with which to visualize data and analyze play performance. You can add the player name column to each of the clustering analysis results and print the list of players by cluster to inform data-driven decisions. You can recreate the barplots with the categorical features names for quickly visual analyses. And, you can recreate the interactive scatterplots that enable you to see where the player names appear on your bi-variate analyses! I hope this worksheet was useful and insightful!"""